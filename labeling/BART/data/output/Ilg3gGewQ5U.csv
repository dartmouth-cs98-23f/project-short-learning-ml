second,categories,transcript,machine learning,computer graphics,web development,algebra,calculus,statistics,economics,marketing,commerce,kinematics,electromagnetism,thermodynamics,geology,cartography,meteorology,genetics,biochemistry,ecology
0,"['machine learning', 'genetics', 'ecology']","Here we tackle backpropagation,",0.7702410817146301,0.422197163105011,0.04706797003746033,0.2368716597557068,0.14198003709316254,0.24450241029262543,0.15452918410301208,0.28844958543777466,0.03987188637256622,0.16152551770210266,0.05301901325583458,0.35918572545051575,0.5252534747123718,0.056475359946489334,0.30223894119262695,0.6324256658554077,0.35289525985717773,0.5730103850364685
5,"['machine learning', 'computer graphics', 'marketing']","Here we tackle backpropagation, the core algorithm behind how neural networks learn. After a quick recap for where we are,",0.6276428699493408,0.055564071983098984,0.019603148102760315,0.014419390819966793,0.01048541534692049,0.016701320186257362,0.003791137132793665,0.05128633603453636,0.01683240942656994,0.01686735637485981,0.005305471830070019,0.0014286631485447288,0.0036471751518547535,0.011623133905231953,0.002886082511395216,0.0032976511865854263,0.004071858245879412,0.008148830384016037
10,"['machine learning', 'marketing', 'kinematics']","the core algorithm behind how neural networks learn. After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough for what the algorithm is actually doing",0.7949467301368713,0.028672125190496445,0.01932530663907528,0.027429096400737762,0.005167721305042505,0.005443057511001825,0.008519629016518593,0.061334580183029175,0.018219254910945892,0.05283987149596214,0.004282352514564991,0.0007130002486519516,0.006526934448629618,0.012417977675795555,0.0010680662235245109,0.0011906498111784458,0.0017332967836409807,0.01386977732181549
15,"['machine learning', 'marketing', 'algebra']","the first thing I'll do is an intuitive walkthrough for what the algorithm is actually doing without any reference to the formulas, Then for those of you who do want to dive into the math,",0.5200564861297607,0.07450553774833679,0.0868055671453476,0.19437412917613983,0.11619474738836288,0.014097599312663078,0.03149021044373512,0.23167206346988678,0.04736349359154701,0.12580077350139618,0.11018980294466019,0.06437412649393082,0.03384092450141907,0.012711751274764538,0.014984405599534512,0.0017121400451287627,0.02465740218758583,0.02104911021888256
20,"['calculus', 'algebra', 'marketing']","without any reference to the formulas, Then for those of you who do want to dive into the math, the next video goes into the calculus underlying all this. If you watched the last two videos",0.10509316623210907,0.026585573330521584,0.03620277717709541,0.30125492811203003,0.47686150670051575,0.0033304684329777956,0.04852462559938431,0.14156395196914673,0.010615823790431023,0.03706308454275131,0.046326156705617905,0.013736053369939327,0.012988908216357231,0.0054444982670247555,0.008528760634362698,0.00245278631336987,0.026628246530890465,0.010523485019803047
25,"['machine learning', 'calculus', 'algebra']","the next video goes into the calculus underlying all this. If you watched the last two videos or if you're just jumping in with the appropriate background, you know what a neural network is and how it feeds forward information.",0.754097580909729,0.06183118373155594,0.15655428171157837,0.5292578339576721,0.6417307257652283,0.01583201065659523,0.17984867095947266,0.2943021059036255,0.020630966871976852,0.306912362575531,0.04071444272994995,0.008163249120116234,0.005752032157033682,0.018452614545822144,0.005883924197405577,0.018873505294322968,0.020125309005379677,0.039980508387088776
30,"['machine learning', 'marketing', 'kinematics']","or if you're just jumping in with the appropriate background, you know what a neural network is and how it feeds forward information. Here we're doing the classic example of recognizing handwritten digits,",0.738837480545044,0.06650829315185547,0.09191355109214783,0.030051667243242264,0.020310895517468452,0.08405351638793945,0.043473925441503525,0.20847852528095245,0.022455384954810143,0.1467253863811493,0.017083628103137016,0.009066103957593441,0.01557986531406641,0.017117192968726158,0.020058603957295418,0.01316893845796585,0.02995302714407444,0.08153793215751648
35,"['machine learning', 'marketing', 'electromagnetism']","Here we're doing the classic example of recognizing handwritten digits, whose pixel values get fed into the first layer of the network with 784 neurons. And I've been showing a network with two hidden layers having just 16 neurons each,",0.33497390151023865,0.028781801462173462,0.014918993227183819,0.01790330372750759,0.012449845671653748,0.015292849391698837,0.006566557101905346,0.0712936669588089,0.00046440778532996774,0.03450382128357887,0.05647410824894905,0.0042016347870230675,0.0029006453696638346,0.010507497936487198,0.004445282742381096,0.020728006958961487,0.0041771866381168365,0.008295504376292229
40,"['machine learning', 'computer graphics', 'web development']","whose pixel values get fed into the first layer of the network with 784 neurons. And I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as its answer.",0.7243691086769104,0.5902155041694641,0.2767624855041504,0.07156596332788467,0.08241405338048935,0.07093865424394608,0.08053150027990341,0.18274833261966705,0.008043358102440834,0.13788114488124847,0.16734200716018677,0.013572449795901775,0.03702529892325401,0.07904153317213058,0.023458588868379593,0.057492051273584366,0.032370708882808685,0.06505177170038223
45,"['machine learning', 'statistics', 'kinematics']","and an output layer of 10 neurons, indicating which digit the network is choosing as its answer.",0.690759003162384,0.3367306590080261,0.2256384789943695,0.17056545615196228,0.2227490097284317,0.41187694668769836,0.1701204478740692,0.27350980043411255,0.08898869901895523,0.36888939142227173,0.1415882408618927,0.04123768210411072,0.12710526585578918,0.16211307048797607,0.019640518352389336,0.025302637368440628,0.09017256647348404,0.14704179763793945
50,"['machine learning', 'geology', 'statistics']","I'm also expecting you to understand gradient descent as described in the last video, and how what we mean by learning is that",0.605671226978302,0.28395792841911316,0.15068717300891876,0.09087473154067993,0.09439649432897568,0.3074783384799957,0.16772690415382385,0.28418946266174316,0.04646029695868492,0.09977646172046661,0.004908418282866478,0.017749454826116562,0.4525964558124542,0.01633474789559841,0.00419563427567482,0.009650718420743942,0.02370413765311241,0.0681181475520134
55,"['machine learning', 'marketing', 'economics']","I'm also expecting you to understand gradient descent as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain cost function.",0.7256020903587341,0.2447068691253662,0.13861697912216187,0.13199642300605774,0.14420808851718903,0.26560577750205994,0.26631608605384827,0.2668561339378357,0.02948327735066414,0.10187933593988419,0.023194996640086174,0.015335633419454098,0.15425778925418854,0.05034121125936508,0.008144622668623924,0.016039123758673668,0.008253510110080242,0.07051697373390198
60,"['machine learning', 'economics', 'statistics']","we want to find which weights and biases minimize a certain cost function. As a quick reminder, for the cost of a single training example,",0.4566122889518738,0.19915418326854706,0.08161550015211105,0.17763787508010864,0.10582912713289261,0.28956499695777893,0.3393017053604126,0.12088380008935928,0.028703760355710983,0.15160627663135529,0.08728569746017456,0.09405236691236496,0.07917409390211105,0.06005363538861275,0.08389931172132492,0.047209158539772034,0.08179394155740738,0.1458801031112671
65,"['machine learning', 'economics', 'marketing']","As a quick reminder, for the cost of a single training example, what you do is take the output that the network gives, along with the output that you wanted it to give,",0.6225740313529968,0.2759111225605011,0.09222976118326187,0.29799291491508484,0.16212297976016998,0.24153803288936615,0.5140153169631958,0.33981189131736755,0.05351628363132477,0.30224311351776123,0.08551543205976486,0.06519404798746109,0.07568974047899246,0.11956001818180084,0.04444470256567001,0.09173371642827988,0.08717329055070877,0.19848713278770447
70,"['machine learning', 'web development', 'economics']","what you do is take the output that the network gives, along with the output that you wanted it to give, and you just add up the squares of the differences between each component.",0.5343146324157715,0.2840439975261688,0.4812108874320984,0.38206738233566284,0.2995401620864868,0.28576016426086426,0.4638543725013733,0.3007761538028717,0.05114959925413132,0.31655070185661316,0.3203398287296295,0.02880057506263256,0.06076134368777275,0.1455923467874527,0.022096719592809677,0.01908843405544758,0.06377028673887253,0.2143896520137787
75,"['statistics', 'computer graphics', 'machine learning']","and you just add up the squares of the differences between each component. Doing this for all of your tens of thousands of training examples, and averaging the results,",0.4364776909351349,0.4511391520500183,0.17273671925067902,0.18040193617343903,0.18895289301872253,0.5213921666145325,0.28563421964645386,0.17321887612342834,0.02144503965973854,0.25577569007873535,0.13474053144454956,0.09381495416164398,0.13741856813430786,0.10989481955766678,0.09771610796451569,0.0892254188656807,0.08810168504714966,0.1338510513305664
80,"['machine learning', 'economics', 'statistics']","Doing this for all of your tens of thousands of training examples, and averaging the results, this gives you the total cost of the network. And as if that's not enough to think about, as described in the last video,",0.611977219581604,0.2838131785392761,0.09557855129241943,0.2394677698612213,0.19634228944778442,0.5951600074768066,0.5992727279663086,0.3127831220626831,0.0348220020532608,0.2198142409324646,0.17720478773117065,0.1042073592543602,0.06255225092172623,0.15554192662239075,0.07620075345039368,0.12113875150680542,0.06307453662157059,0.12714092433452606
85,"['machine learning', 'calculus', 'economics']","this gives you the total cost of the network. And as if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this cost function,",0.5603571534156799,0.2176913321018219,0.20947352051734924,0.3463246822357178,0.5501713156700134,0.4038948714733124,0.4604434072971344,0.33598825335502625,0.05902532488107681,0.3304762840270996,0.19077111780643463,0.06796083599328995,0.2471504509449005,0.10827281326055527,0.008474919013679028,0.07089587301015854,0.008867649361491203,0.2025623917579651
90,"['machine learning', 'economics', 'algebra']","the thing that we're looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections,",0.5800893902778625,0.22813329100608826,0.17047929763793945,0.3673006594181061,0.33085736632347107,0.35303041338920593,0.43071186542510986,0.3191601634025574,0.056893523782491684,0.3372512757778168,0.08437291532754898,0.01847236603498459,0.1320200115442276,0.06029876321554184,0.005852462723851204,0.007635804358869791,0.030770236626267433,0.07295012474060059
95,"['economics', 'web development', 'algebra']","which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost.",0.47958534955978394,0.47100239992141724,0.528842568397522,0.49256595969200134,0.39545977115631104,0.4138728082180023,0.5695836544036865,0.3723924458026886,0.08187692612409592,0.24250246584415436,0.1348114311695099,0.07326161116361618,0.08994053304195404,0.11902161687612534,0.020053943619132042,0.09314829111099243,0.07782284170389175,0.10464512556791306
100,"['economics', 'machine learning', 'computer graphics']","so as to most efficiently decrease the cost. Backpropagation, the topic of this video,",0.34873056411743164,0.34387126564979553,0.1802525669336319,0.06690400093793869,0.0833410918712616,0.1768673062324524,0.36361435055732727,0.2912098169326782,0.10628975182771683,0.08202531188726425,0.023102398961782455,0.022976186126470566,0.02432136982679367,0.02366252802312374,0.023510389029979706,0.014524860307574272,0.026673225685954094,0.04587678238749504
105,"['machine learning', 'computer graphics', 'marketing']","Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient. And the one idea from the last video that I really want you to hold firmly in your mind right now",0.6411474347114563,0.266691654920578,0.09508595615625381,0.09071055054664612,0.10842407494783401,0.09723860025405884,0.059606656432151794,0.13010677695274353,0.010676690377295017,0.07864324748516083,0.007238483056426048,0.004893525969237089,0.054088085889816284,0.01101840753108263,0.017380762845277786,0.014447866007685661,0.03360636904835701,0.03895845636725426
110,"['machine learning', 'computer graphics', 'marketing']","is an algorithm for computing that crazy complicated gradient. And the one idea from the last video that I really want you to hold firmly in your mind right now is that because thinking of the gradient vector as a direction in 13000 dimensions is,",0.7165302038192749,0.494332879781723,0.10606224834918976,0.1821783185005188,0.20600678026676178,0.2021261602640152,0.12996789813041687,0.3115464150905609,0.018061434850096703,0.22691534459590912,0.0925406962633133,0.06422649323940277,0.2781727612018585,0.1469365805387497,0.025544535368680954,0.04954614117741585,0.05238916352391243,0.1906193494796753
115,"['geology', 'marketing', 'computer graphics']","is that because thinking of the gradient vector as a direction in 13000 dimensions is, to put it lightly, beyond the scope of our imaginations,",0.028118140995502472,0.12674959003925323,0.0390051044523716,0.10845983773469925,0.06589904427528381,0.023906053975224495,0.03090350516140461,0.19926205277442932,0.06576931476593018,0.09173464775085449,0.02961527556180954,0.0269940048456192,0.2622649371623993,0.06899134814739227,0.0141736576333642,0.00807590689510107,0.010606457479298115,0.06005081161856651
120,"['marketing', 'ecology', 'machine learning']","to put it lightly, beyond the scope of our imaginations, there's another way you can think about it: The magnitude of each component here is telling you",0.3490462899208069,0.30731865763664246,0.23611770570278168,0.21460147202014923,0.08041860908269882,0.19871847331523895,0.2775363624095917,0.43189480900764465,0.14050500094890594,0.20725548267364502,0.19203689694404602,0.1989952027797699,0.2198018580675125,0.18604473769664764,0.16808746755123138,0.1896248608827591,0.3172195255756378,0.3626422882080078
125,"['economics', 'algebra', 'kinematics']",there's another way you can think about it: The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias.,0.4641079008579254,0.4420076608657837,0.30914250016212463,0.5753490924835205,0.3173102140426636,0.2998768985271454,0.5881801247596741,0.3332514464855194,0.0945848673582077,0.5254724621772766,0.15771745145320892,0.17008307576179504,0.1180325597524643,0.243208646774292,0.057061053812503815,0.08747203648090363,0.2738552391529083,0.30221083760261536
130,"['machine learning', 'computer graphics', 'economics']","how sensitive the cost function is to each weight and bias. For example, let's say you go through the process I'm about to describe, and you compute the negative gradient,",0.7276887893676758,0.5625181198120117,0.23411111533641815,0.4871075749397278,0.48621585965156555,0.4876115918159485,0.5259532928466797,0.343129962682724,0.04076889529824257,0.3534282445907593,0.16942225396633148,0.09653817117214203,0.10753712803125381,0.10084405541419983,0.08850415796041489,0.11070670932531357,0.13887274265289307,0.14836540818214417
135,"['machine learning', 'computer graphics', 'calculus']","For example, let's say you go through the process I'm about to describe, and you compute the negative gradient, and the component associated with the weight on this edge here comes out to be 3.2,",0.7427704334259033,0.573085606098175,0.28707659244537354,0.4982101023197174,0.572952389717102,0.48664239048957825,0.4455631375312805,0.3911353349685669,0.028864223510026932,0.47515252232551575,0.2605885863304138,0.20439793169498444,0.2491355985403061,0.16308870911598206,0.05383789911866188,0.14399638772010803,0.18827740848064423,0.18056030571460724
140,"['web development', 'machine learning', 'computer graphics']","and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with this edge here comes out as 0.1.",0.2636236250400543,0.2469213306903839,0.46124163269996643,0.0703422874212265,0.14667046070098877,0.14780627191066742,0.13810475170612335,0.16154706478118896,0.01073872298002243,0.08542975783348083,0.06379210948944092,0.03819975629448891,0.10488069802522659,0.11584348976612091,0.03660985082387924,0.06447658687829971,0.029947903007268906,0.07507126033306122
145,"['calculus', 'economics', 'machine learning']",while the component associated with this edge here comes out as 0.1. The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight.,0.5634932518005371,0.46706336736679077,0.4383282959461212,0.4180528521537781,0.7277249693870544,0.417440265417099,0.6863436698913574,0.4193732738494873,0.02589632198214531,0.3383539915084839,0.15418702363967896,0.20020239055156708,0.23458288609981537,0.1619308888912201,0.03971823304891586,0.1810550093650818,0.1534232497215271,0.1307193785905838
150,"['economics', 'machine learning', 'calculus']","The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight. So if you were to wiggle that value just a little bit,",0.6473487019538879,0.3656919300556183,0.19390510022640228,0.48654988408088684,0.5693091750144958,0.4964368939399719,0.7662901878356934,0.3856227397918701,0.07131075114011765,0.5130927562713623,0.23493246734142303,0.17791402339935303,0.16516375541687012,0.2866092324256897,0.06859227269887924,0.19386298954486847,0.27536678314208984,0.18756909668445587
155,"['economics', 'statistics', 'calculus']","So if you were to wiggle that value just a little bit, it's gonna cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give.",0.34208759665489197,0.3016781508922577,0.3130560517311096,0.40567365288734436,0.56500244140625,0.6047776341438293,0.6891245245933533,0.4367062747478485,0.0930975005030632,0.46731752157211304,0.1999291479587555,0.1962776631116867,0.18268421292304993,0.29120656847953796,0.04741556569933891,0.1796581894159317,0.177715465426445,0.23299162089824677
160,"['statistics', 'economics', 'calculus']","it's gonna cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give.",0.4025813341140747,0.3699173927307129,0.20576566457748413,0.3461228907108307,0.5061126947402954,0.6063780784606934,0.5964321494102478,0.43949106335639954,0.1407899558544159,0.3551589250564575,0.21768340468406677,0.20415420830249786,0.17460228502750397,0.30515432357788086,0.05361359938979149,0.177262082695961,0.19668032228946686,0.17380104959011078
165,"['machine learning', 'marketing', 'genetics']","Personally, when I was first learning about backpropagation,",0.6959607005119324,0.17156918346881866,0.029982278123497963,0.01963905058801174,0.016140077263116837,0.11147899925708771,0.19434480369091034,0.4251249432563782,0.05856817960739136,0.14417684078216553,0.08621306717395782,0.12217891216278076,0.33520299196243286,0.07169200479984283,0.19675317406654358,0.41835513710975647,0.16898313164710999,0.36357250809669495
170,"['machine learning', 'computer graphics', 'marketing']","Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and the index chasing of it all.",0.4784982204437256,0.3392546474933624,0.16979286074638367,0.2288789302110672,0.12116698920726776,0.13824506103992462,0.20399592816829681,0.30249762535095215,0.03164547681808472,0.16217546164989471,0.13694514334201813,0.03961166739463806,0.2923659384250641,0.07794691622257233,0.10465993732213974,0.24776794016361237,0.1598578542470932,0.2939492464065552
175,"['machine learning', 'marketing', 'computer graphics']","I think the most confusing aspect was just the notation and the index chasing of it all. But once you unwrap what each part of this algorithm is really doing, each individual effect that it's having is actually pretty intuitive.",0.4702516496181488,0.4352084994316101,0.2890383005142212,0.4069691002368927,0.2984437346458435,0.28313103318214417,0.3836745619773865,0.4616014063358307,0.0376492477953434,0.3079061210155487,0.23258540034294128,0.15384304523468018,0.15886884927749634,0.09699078649282455,0.08233457803726196,0.11490339785814285,0.1345261186361313,0.15941494703292847
180,"['computer graphics', 'marketing', 'machine learning']","But once you unwrap what each part of this algorithm is really doing, each individual effect that it's having is actually pretty intuitive. It's just that there's a lot of little adjustments getting layered on top of each other.",0.44531601667404175,0.49866783618927,0.16110225021839142,0.32654404640197754,0.264848917722702,0.2385133057832718,0.3285301625728607,0.479938268661499,0.049207206815481186,0.2074446976184845,0.20052401721477509,0.14497071504592896,0.12690973281860352,0.1785963773727417,0.08525718748569489,0.06981237232685089,0.1079113557934761,0.19325807690620422
185,"['web development', 'marketing', 'computer graphics']","It's just that there's a lot of little adjustments getting layered on top of each other. So I'm gonna start things off here with a complete disregard for the notation,",0.23119652271270752,0.23147721588611603,0.3529435992240906,0.09298484772443771,0.03200463205575943,0.049075961112976074,0.19040006399154663,0.25891074538230896,0.016071680933237076,0.015057946555316448,0.09184466302394867,0.0722670629620552,0.13095703721046448,0.0611109733581543,0.08006671071052551,0.05519181489944458,0.12233219295740128,0.08472619205713272
190,"['machine learning', 'marketing', 'statistics']","So I'm gonna start things off here with a complete disregard for the notation, and just step through those effects that each training example is having on the weights and biases.",0.5795719027519226,0.24838374555110931,0.07311689108610153,0.023356206715106964,0.19959726929664612,0.2703896462917328,0.12327983975410461,0.342126727104187,0.0947355329990387,0.15014788508415222,0.1613793671131134,0.09063126891851425,0.1359100639820099,0.03224458545446396,0.0453876256942749,0.13366834819316864,0.21666692197322845,0.14088568091392517
195,"['machine learning', 'statistics', 'economics']","and just step through those effects that each training example is having on the weights and biases. Because the cost function involves averaging a certain cost per example over all the tens of thousands of training examples,",0.6057103872299194,0.3160887658596039,0.11503291875123978,0.1418430358171463,0.13388429582118988,0.36658579111099243,0.3629254996776581,0.23424431681632996,0.03371135890483856,0.19166772067546844,0.12655285000801086,0.051114898175001144,0.08180257678031921,0.09591636061668396,0.05059387534856796,0.10281462967395782,0.04505987465381622,0.18820089101791382
200,"['machine learning', 'economics', 'marketing']","Because the cost function involves averaging a certain cost per example over all the tens of thousands of training examples, the way that we adjust the weights and biases for a single gradient descent step",0.6409827470779419,0.23972098529338837,0.092007577419281,0.2241770327091217,0.19826219975948334,0.27268946170806885,0.47061818838119507,0.2901253402233124,0.016516879200935364,0.11938469856977463,0.05593372881412506,0.030044347047805786,0.05450055003166199,0.054326627403497696,0.015601293183863163,0.014611947350203991,0.0273948572576046,0.07561331987380981
205,"['machine learning', 'computer graphics', 'statistics']","the way that we adjust the weights and biases for a single gradient descent step also depends on every single example,",0.5893338322639465,0.3567139208316803,0.08808496594429016,0.3091658353805542,0.18534386157989502,0.3530164957046509,0.19177071750164032,0.2174379527568817,0.02323424071073532,0.08862218260765076,0.10263321548700333,0.05194533243775368,0.10210737586021423,0.030961835756897926,0.030793482437729836,0.014779255725443363,0.02783012017607689,0.046539057046175
210,"['machine learning', 'computer graphics', 'web development']","also depends on every single example, or rather in principle it should, but for computational efficiency we're going to do a little trick later",0.5474370718002319,0.5429244041442871,0.32738083600997925,0.2173374742269516,0.13797900080680847,0.15186412632465363,0.20610709488391876,0.17866398394107819,0.10526309907436371,0.12505412101745605,0.09908395260572433,0.07128267735242844,0.04192107170820236,0.02902817539870739,0.0321354903280735,0.0143668781965971,0.05910777300596237,0.10061676055192947
215,"['machine learning', 'web development', 'computer graphics']","or rather in principle it should, but for computational efficiency we're going to do a little trick later to keep you from needing to hit every single example for every single step. Another case right now,",0.2734454870223999,0.2230415940284729,0.2270011454820633,0.1599731594324112,0.12070409953594208,0.06798543781042099,0.04767100512981415,0.10799573361873627,0.02856585942208767,0.041733577847480774,0.05399077758193016,0.040480103343725204,0.03276553004980087,0.01738133281469345,0.011449734680354595,0.03075798787176609,0.0344778448343277,0.022525494918227196
220,"['biochemistry', 'computer graphics', 'kinematics']","to keep you from needing to hit every single example for every single step. Another case right now, all we're gonna do is focus our attention on one single example: this image of a 2.",0.3251872956752777,0.35871464014053345,0.21544249355793,0.3100111484527588,0.2295503467321396,0.15376698970794678,0.23807881772518158,0.30371594429016113,0.041891857981681824,0.3537263572216034,0.1698988676071167,0.20631657540798187,0.24578560888767242,0.10930472612380981,0.10966528207063675,0.2082345336675644,0.3685441017150879,0.1525399386882782
225,"['machine learning', 'computer graphics', 'kinematics']",all we're gonna do is focus our attention on one single example: this image of a 2. What effect should this one training example have on how the weights and biases get adjusted?,0.5334352850914001,0.46913886070251465,0.09695518761873245,0.37662753462791443,0.2735739052295685,0.3614881634712219,0.24656051397323608,0.29891452193260193,0.14310358464717865,0.4042677581310272,0.21135978400707245,0.19727493822574615,0.16839276254177094,0.31395184993743896,0.19961366057395935,0.21770963072776794,0.2953796982765198,0.20721150934696198
230,"['machine learning', 'statistics', 'kinematics']","What effect should this one training example have on how the weights and biases get adjusted? Let's say we're at a point where the network is not well trained yet,",0.7059659957885742,0.6115059852600098,0.2193199098110199,0.4969986081123352,0.560185968875885,0.6546879410743713,0.5144588947296143,0.41623249650001526,0.15885266661643982,0.6264382004737854,0.3038471043109894,0.25887081027030945,0.3694702088832855,0.16784779727458954,0.2694477438926697,0.271674782037735,0.2826738953590393,0.2241547852754593
235,"['machine learning', 'computer graphics', 'marketing']","Let's say we're at a point where the network is not well trained yet, so the activations in the output are gonna look pretty random, maybe something like 0.5, 0.8, 0.2, on and on.",0.6457573175430298,0.5146983861923218,0.30785661935806274,0.17953993380069733,0.18009807169437408,0.3795997202396393,0.257826030254364,0.42162764072418213,0.04372687637805939,0.2576681971549988,0.1697721630334854,0.09378514438867569,0.22473758459091187,0.26970866322517395,0.06453420966863632,0.20285025238990784,0.08630599081516266,0.2315485179424286
240,"['computer graphics', 'machine learning', 'genetics']","so the activations in the output are gonna look pretty random, maybe something like 0.5, 0.8, 0.2, on and on. Now we can't directly change those activations, we only have influence on the weights and biases,",0.37088820338249207,0.5109778046607971,0.3237755000591278,0.33837008476257324,0.2948512136936188,0.34477484226226807,0.3147161900997162,0.3057462275028229,0.02906412072479725,0.3201296627521515,0.2351190447807312,0.143032044172287,0.2768813669681549,0.31629592180252075,0.24816131591796875,0.36727118492126465,0.2229640632867813,0.30493560433387756
245,"['machine learning', 'economics', 'computer graphics']","Now we can't directly change those activations, we only have influence on the weights and biases, but it is helpful to keep track of which adjustments we wish should take place to that output layer,",0.5360926985740662,0.49070876836776733,0.16871041059494019,0.4293246865272522,0.33612555265426636,0.4077302813529968,0.4998800754547119,0.40229249000549316,0.16075637936592102,0.4835796058177948,0.20888389647006989,0.17523369193077087,0.09641074389219284,0.18628470599651337,0.06930956989526749,0.15502679347991943,0.20285217463970184,0.10236215591430664
250,"['computer graphics', 'kinematics', 'algebra']","but it is helpful to keep track of which adjustments we wish should take place to that output layer, and since we want it to classify the image as a 2,",0.5658597350120544,0.671004056930542,0.4353552460670471,0.5777935981750488,0.47530633211135864,0.4294850528240204,0.3460659384727478,0.2919091582298279,0.1286439746618271,0.6083217859268188,0.181462362408638,0.17592079937458038,0.25189805030822754,0.15591371059417725,0.10328049957752228,0.10806699842214584,0.1444038450717926,0.19341513514518738
255,"['computer graphics', 'machine learning', 'web development']","and since we want it to classify the image as a 2, we want that third value to get nudged up, while all of the others get nudged down.",0.601479709148407,0.7134225368499756,0.579833447933197,0.3305904269218445,0.4337623119354248,0.4323591887950897,0.48474621772766113,0.5130589008331299,0.07568370550870895,0.514173150062561,0.27198097109794617,0.14958935976028442,0.238124281167984,0.2748025953769684,0.08520042896270752,0.18155710399150848,0.15315955877304077,0.1977856308221817
260,"['economics', 'machine learning', 'computer graphics']","we want that third value to get nudged up, while all of the others get nudged down. Moreover, the sizes of these nudges should be proportional to",0.6705629825592041,0.605820894241333,0.3998453915119171,0.4966962933540344,0.40283745527267456,0.5637872815132141,0.7066590785980225,0.567663848400116,0.12300921976566315,0.3951728940010071,0.3140844404697418,0.45936647057533264,0.42449378967285156,0.4188026785850525,0.19664397835731506,0.5459489226341248,0.3281504809856415,0.375520259141922
265,"['economics', 'calculus', 'machine learning']","Moreover, the sizes of these nudges should be proportional to how far away each current value is from its target value.",0.5348867177963257,0.48566824197769165,0.15705111622810364,0.47510287165641785,0.536550760269165,0.44360414147377014,0.5850338339805603,0.4385426342487335,0.05896769091486931,0.4625145494937897,0.26395997405052185,0.29846784472465515,0.258162260055542,0.45861873030662537,0.1265639364719391,0.14955423772335052,0.17653433978557587,0.25887659192085266
270,"['machine learning', 'statistics', 'biochemistry']","how far away each current value is from its target value. For example, the increase to that number 2 neurons activation is, in a sense, more important than the decrease to the number 8 neuron,",0.5933298468589783,0.1210828348994255,0.08507276326417923,0.1368478387594223,0.18441490828990936,0.37482360005378723,0.19765008985996246,0.2583732604980469,0.03276197239756584,0.19670411944389343,0.17371320724487305,0.04792063683271408,0.04388916864991188,0.04201889410614967,0.0248037651181221,0.12711672484874725,0.29250308871269226,0.15111568570137024
275,"['machine learning', 'biochemistry', 'kinematics']","For example, the increase to that number 2 neurons activation is, in a sense, more important than the decrease to the number 8 neuron, which is already pretty close to where it should be.",0.6056011319160461,0.2220015972852707,0.15928401052951813,0.3623174726963043,0.28679296374320984,0.49106407165527344,0.2802169919013977,0.4245137572288513,0.1032135933637619,0.5043475031852722,0.2565036118030548,0.18559043109416962,0.1169104129076004,0.28572314977645874,0.02224014885723591,0.35536590218544006,0.5762133598327637,0.4059537947177887
280,"['machine learning', 'computer graphics', 'biochemistry']","which is already pretty close to where it should be. So zooming in further, let's focus just on this one neuron,",0.5261546969413757,0.49672868847846985,0.19945128262043,0.022916294634342194,0.03228912502527237,0.31032946705818176,0.10970251262187958,0.2779245972633362,0.03381142392754555,0.11508835852146149,0.2537434697151184,0.08175128698348999,0.07196423411369324,0.17629657685756683,0.03136493265628815,0.197573721408844,0.35070618987083435,0.23811647295951843
285,"['machine learning', 'statistics', 'kinematics']","So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase. Remember, that activation is defined as",0.6548910140991211,0.3959634304046631,0.16385194659233093,0.20935460925102234,0.1142554059624672,0.43770933151245117,0.1784297376871109,0.2903764843940735,0.18675240874290466,0.4362500309944153,0.36008384823799133,0.08216123282909393,0.06245335936546326,0.2830335795879364,0.07649420946836472,0.199361190199852,0.42693817615509033,0.3182491660118103
290,"['machine learning', 'calculus', 'statistics']","the one whose activation we wish to increase. Remember, that activation is defined as a certain weighted sum of all of the activations in the previous layer, plus a bias,",0.6110097765922546,0.491473913192749,0.25259265303611755,0.38363268971443176,0.5144580602645874,0.4983501136302948,0.36786913871765137,0.2881716787815094,0.04980635270476341,0.34673774242401123,0.18254698812961578,0.1437072902917862,0.30371659994125366,0.17580725252628326,0.13511495292186737,0.24270498752593994,0.21108046174049377,0.29854291677474976
295,"['machine learning', 'computer graphics', 'statistics']","a certain weighted sum of all of the activations in the previous layer, plus a bias, which has all been plugged into something like the sigmoid squishification function or a ReLU,",0.7516952753067017,0.5014239549636841,0.12265601009130478,0.3123636841773987,0.25631552934646606,0.49741965532302856,0.18549349904060364,0.30273541808128357,0.08113142848014832,0.20336273312568665,0.10309438407421112,0.04242466390132904,0.05939241126179695,0.04348132386803627,0.01992557942867279,0.017233990132808685,0.07660949975252151,0.031361885368824005
