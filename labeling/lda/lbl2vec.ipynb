{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lbl2Vec Model\n",
    "\n",
    "## LDA Topic Inference\n",
    "\n",
    "Unsupervised text classification model. Requires sort of a dataset that can map terms to the correct clusters.\n",
    "Can be done manually during the training stage of the LDA. But implementing this in the case where it might out-perform human input (such as long topic lists).\n",
    "There has also been a paper showing that such models can perform better than LDAs; so if effective, the Lbl2vec model can be used for topic modeling directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups     # the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vars here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train = fetch_20newsgroups(subset='train', shuffle=False)\n",
    "test = fetch_20newsgroups(subset='test', shuffle=False)\n",
    "\n",
    "# parse data to pandas DataFrames\n",
    "newsgroup_train = pd.DataFrame({'article':train.data, 'class_index':train.target})\n",
    "newsgroup_test = pd.DataFrame({'article':test.data, 'class_index':test.target})\n",
    "\n",
    "# load labels with keywords\n",
    "labels = pd.read_csv('20newsgroups_keywords.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   class_index          class_name               keywords  number_of_keywords\n",
      "0            8     rec.motorcycles    [bikes, motorcycle]                   2\n",
      "1            9  rec.sport.baseball             [baseball]                   1\n",
      "2           10    rec.sport.hockey               [hockey]                   1\n",
      "3           11           sci.crypt  [encryption, privacy]                   2\n"
     ]
    }
   ],
   "source": [
    "# data pre-processing\n",
    "\n",
    "# split keywords by separator and save them as array\n",
    "labels['keywords'] = labels['keywords'].apply(lambda x: x.split(' '))\n",
    "\n",
    "# convert description keywords to lowercase\n",
    "labels['keywords'] = labels['keywords'].apply(lambda description_keywords: [keyword.lower() for keyword in description_keywords])\n",
    "\n",
    "# get number of keywords for each class\n",
    "labels['number_of_keywords'] = labels['keywords'].apply(lambda row: len(row))\n",
    "\n",
    "# lets check our keywords\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             article  class_index   \n",
      "0  From: cubbie@garnet.berkeley.edu (            ...            9  \\\n",
      "1  From: gnelson@pion.rutgers.edu (Gregory Nelson...            4   \n",
      "2  From: crypt-comments@math.ncsu.edu\\nSubject: C...           11   \n",
      "3  From:  ()\\nSubject: Re: Quadra SCSI Problems??...            4   \n",
      "4  From: keith@cco.caltech.edu (Keith Allan Schne...            0   \n",
      "\n",
      "  data_set_type  \n",
      "0         train  \n",
      "1         train  \n",
      "2         train  \n",
      "3         train  \n",
      "4         train  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import strip_tags\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "# doc: document text string\n",
    "# returns tokenized document\n",
    "# strip_tags removes meta tags from the text\n",
    "# simple preprocess converts a document into a list of lowercase tokens, ignoring tokens that are too short or too long \n",
    "# simple preprocess also removes numerical values as well as punktuation characters\n",
    "def tokenize(doc):\n",
    "    return simple_preprocess(strip_tags(doc), deacc=True, min_len=2, max_len=15)\n",
    "\n",
    "# add data set type column\n",
    "newsgroup_train['data_set_type'] = 'train'\n",
    "newsgroup_test['data_set_type'] = 'test'\n",
    "\n",
    "# concat train and test data\n",
    "newsgroup_full_corpus = pd.concat([newsgroup_train,newsgroup_test]).reset_index(drop=True)\n",
    "\n",
    "# reduce dataset to only articles that belong to classes where we defined our keywords\n",
    "newsgroup_full_corpus = newsgroup_full_corpus[newsgroup_full_corpus['class_index'].isin(list(labels['class_index']))]\n",
    "\n",
    "# tokenize and tag documents for Lbl2Vec training\n",
    "newsgroup_full_corpus['tagged_docs'] = newsgroup_full_corpus.apply(lambda row: TaggedDocument(tokenize(row['article']), \n",
    "    [str(row.name)]), axis=1)\n",
    "\n",
    "# add doc_key column\n",
    "newsgroup_full_corpus['doc_key'] = newsgroup_full_corpus.index.astype(str)\n",
    "\n",
    "# add class_name column\n",
    "newsgroup_full_corpus = newsgroup_full_corpus.merge(labels, left_on='class_index', right_on='class_index', how='left')\n",
    "print(newsgroup_full_corpus.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7afd7476676b4429b6e5f6a42b5d8385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75be3cc1b5124f6aba8a5e8aacce7c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13422cb79a4b476086b94460904c9d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb68f2ee1554b1f829fb6fb94479bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)55de9125/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d928ea9679647f89202e94fc9657ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab8449a16024fefb0d62e52789c2f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)125/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bdd50a461494c26b296c5c19d4a911c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e493bcae77e640e78bf23357a512b0d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e89f2b83904155994355d7ceedc838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28fce593646349b8b6e9627970f32393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842d388f3554495c92df312b7b956c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8bc47ed483f456bb9a870deea5e4927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)9125/train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25cc0e3c564463f9e32c595e641ccde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6313d788026e41f8a93c7cb3ebce90a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)5de9125/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 11:04:29,314 - Lbl2Vec - INFO - Train document and word embeddings\n",
      "2023-11-05 11:04:34,228 - Lbl2Vec - INFO - Train label embeddings\n"
     ]
    }
   ],
   "source": [
    "from lbl2vec import Lbl2Vec\n",
    "\n",
    "# init model with parameters\n",
    "Lbl2Vec_model = Lbl2Vec(keywords_list=list(labels.keywords), tagged_documents=newsgroup_full_corpus['tagged_docs'][newsgroup_full_corpus['data_set_type'] == 'train'], label_names=list(labels.class_name), similarity_threshold=0.43, min_num_docs=100, epochs=10)\n",
    "\n",
    "# train model\n",
    "Lbl2Vec_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 11:06:34,754 - Lbl2Vec - INFO - Get document embeddings from model\n",
      "2023-11-05 11:06:34,757 - Lbl2Vec - INFO - Calculate document<->label similarities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.9108786610878661\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# predict similarity scores\n",
    "model_docs_lbl_similarities = Lbl2Vec_model.predict_model_docs()\n",
    "\n",
    "# merge DataFrames to compare the predicted and true category labels\n",
    "evaluation_train = model_docs_lbl_similarities.merge(newsgroup_full_corpus[newsgroup_full_corpus['data_set_type'] == 'train'], left_on='doc_key', right_on='doc_key')\n",
    "y_true_train = evaluation_train['class_name']\n",
    "y_pred_train = evaluation_train['most_similar_label']\n",
    "\n",
    "print('F1 score:',f1_score(y_true_train, y_pred_train, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 11:06:40,184 - Lbl2Vec - INFO - Calculate document embeddings\n",
      "2023-11-05 11:06:41,092 - Lbl2Vec - INFO - Calculate document<->label similarities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8867924528301887\n"
     ]
    }
   ],
   "source": [
    "# predict similarity scores of new test documents (they were not used during Lbl2Vec training)\n",
    "new_docs_lbl_similarities = Lbl2Vec_model.predict_new_docs(tagged_docs=newsgroup_full_corpus['tagged_docs'][newsgroup_full_corpus['data_set_type']=='test'])\n",
    "\n",
    "# merge DataFrames to compare the predicted and true topic labels\n",
    "evaluation_test = new_docs_lbl_similarities.merge(newsgroup_full_corpus[newsgroup_full_corpus['data_set_type']=='test'], left_on='doc_key', right_on='doc_key')\n",
    "y_true_test = evaluation_test['class_name']\n",
    "y_pred_test = evaluation_test['most_similar_label']\n",
    "\n",
    "print('F1 score:',f1_score(y_true_test, y_pred_test, average='micro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
