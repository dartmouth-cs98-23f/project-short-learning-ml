{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim LDA\n",
    "\n",
    "Adapted from https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file = 'abcnews-date-text.csv'      # the input csv file\n",
    "topic_file = 'topics_'+csv_file\n",
    "\n",
    "data_path = 'data/'+csv_file\n",
    "topic_path = 'topics/'+topic_file\n",
    "\n",
    "df = pd.read_csv(data_path);\n",
    "df = df[['headline_text']]\n",
    "df['index'] = df.index\n",
    "documents = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1244184\n",
      "                                       headline_text  index\n",
      "0  aba decides against community broadcasting lic...      0\n",
      "1     act fire witnesses must be aware of defamation      1\n",
      "2     a g calls for infrastructure protection summit      2\n",
      "3           air nz staff in aust strike for pay rise      3\n",
      "4      air nz strike to affect australian travellers      4\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/bansharee/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "stemmer = SnowballStemmer(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words in third person are changed to first person and verbs in past and future tenses are changed into present\n",
    "# words are reduced to their root form\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['ratepayers', 'group', 'wants', 'compulsory', 'local', 'govt', 'voting']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['ratepay', 'group', 'want', 'compulsori', 'local', 'govt', 'vote']\n"
     ]
    }
   ],
   "source": [
    "# displaying how preprocessing works\n",
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            [decid, communiti, broadcast, licenc]\n",
       "1                               [wit, awar, defam]\n",
       "2           [call, infrastructur, protect, summit]\n",
       "3                      [staff, aust, strike, rise]\n",
       "4             [strike, affect, australian, travel]\n",
       "5               [ambiti, olsson, win, tripl, jump]\n",
       "6           [antic, delight, record, break, barca]\n",
       "7    [aussi, qualifi, stosur, wast, memphi, match]\n",
       "8            [aust, address, secur, council, iraq]\n",
       "9                         [australia, lock, timet]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess 'headline_text' text from training set\n",
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 communiti\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n",
      "10 summit\n"
     ]
    }
   ],
   "source": [
    "# creating dictionary using words in the training set, mapped to how many times the word appears in the set\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering out:\n",
    "#   * less than 15 documents (absolute number) or\n",
    "#   * more than 0.5 documents (fraction of total corpus size, not absolute number)\n",
    "#   * after the above two steps, keep only the first 100000 most frequent tokens\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1), (2, 1), (3, 1)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[0]\n",
    "\n",
    "# bow_corpus = []\n",
    "# for doc in processed_docs:\n",
    "#     bow = dictionary.doc2bow(doc)\n",
    "#     bow_corpus.append(bow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 4 (\"awar\") appears 1 time.\n",
      "Word 5 (\"defam\") appears 1 time.\n",
      "Word 6 (\"wit\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(\n",
    "                                                    bow_doc_4310[i][0], \n",
    "                                                    dictionary[bow_doc_4310[i][0]], \n",
    "                                                    bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5844216176085719),\n",
      " (1, 0.38716866963787633),\n",
      " (2, 0.5013820927104505),\n",
      " (3, 0.5071171375845095)]\n",
      "broadcast 0.5844216176085719\n",
      "communiti 0.38716866963787633\n",
      "decid 0.5013820927104505\n",
      "licenc 0.5071171375845095\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF weights words based on how often they appear in a document \n",
    "# versus how often they appear in the entire corpus\n",
    "# this helps LDA distinguish topics by weighting more important words higher\n",
    "\n",
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "\n",
    "# preview of how this works\n",
    "pprint(corpus_tfidf[0])\n",
    "for i in range(len(bow_corpus[0])):\n",
    "    print(dictionary[bow_corpus[0][i][0]], corpus_tfidf[0][i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: HDP goes here for num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Words: 0.047*\"polic\" + 0.025*\"death\" + 0.023*\"charg\" + 0.019*\"murder\" + 0.018*\"kill\" + 0.016*\"crash\" + 0.016*\"alleg\" + 0.016*\"attack\" + 0.016*\"woman\" + 0.015*\"border\"\n",
      "Topic: 1 Words: 0.054*\"australian\" + 0.034*\"govern\" + 0.023*\"live\" + 0.023*\"health\" + 0.023*\"news\" + 0.023*\"victoria\" + 0.022*\"nation\" + 0.018*\"adelaid\" + 0.010*\"show\" + 0.010*\"servic\"\n",
      "Topic: 2 Words: 0.048*\"trump\" + 0.039*\"year\" + 0.034*\"elect\" + 0.012*\"presid\" + 0.012*\"fall\" + 0.012*\"say\" + 0.011*\"labor\" + 0.010*\"liber\" + 0.010*\"vote\" + 0.009*\"parti\"\n",
      "Topic: 3 Words: 0.030*\"melbourn\" + 0.026*\"test\" + 0.019*\"restrict\" + 0.019*\"tasmania\" + 0.017*\"lockdown\" + 0.017*\"perth\" + 0.015*\"coronavirus\" + 0.013*\"royal\" + 0.013*\"care\" + 0.013*\"open\"\n",
      "Topic: 4 Words: 0.019*\"plan\" + 0.017*\"hous\" + 0.015*\"rise\" + 0.014*\"region\" + 0.012*\"council\" + 0.012*\"brisban\" + 0.011*\"interview\" + 0.011*\"farm\" + 0.010*\"resid\" + 0.010*\"budget\"\n",
      "Topic: 5 Words: 0.043*\"queensland\" + 0.028*\"china\" + 0.022*\"report\" + 0.016*\"north\" + 0.015*\"worker\" + 0.015*\"indigen\" + 0.012*\"premier\" + 0.011*\"street\" + 0.011*\"claim\" + 0.011*\"communiti\"\n",
      "Topic: 6 Words: 0.022*\"coast\" + 0.019*\"feder\" + 0.016*\"scott\" + 0.014*\"gold\" + 0.013*\"tasmanian\" + 0.013*\"lose\" + 0.011*\"need\" + 0.011*\"game\" + 0.010*\"concern\" + 0.010*\"polit\"\n",
      "Topic: 7 Words: 0.029*\"record\" + 0.026*\"donald\" + 0.020*\"warn\" + 0.017*\"coronavirus\" + 0.016*\"island\" + 0.016*\"morrison\" + 0.015*\"time\" + 0.014*\"high\" + 0.014*\"victoria\" + 0.013*\"west\"\n",
      "Topic: 8 Words: 0.024*\"world\" + 0.021*\"school\" + 0.021*\"market\" + 0.020*\"women\" + 0.017*\"busi\" + 0.016*\"australia\" + 0.014*\"speak\" + 0.013*\"win\" + 0.012*\"final\" + 0.012*\"countri\"\n",
      "Topic: 9 Words: 0.063*\"covid\" + 0.034*\"coronavirus\" + 0.025*\"case\" + 0.023*\"vaccin\" + 0.018*\"chang\" + 0.016*\"peopl\" + 0.014*\"victorian\" + 0.013*\"australia\" + 0.011*\"life\" + 0.011*\"say\"\n"
     ]
    }
   ],
   "source": [
    "# training the model using the bow corpus\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} Words: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.016*\"countri\" + 0.011*\"hour\" + 0.008*\"thursday\" + 0.007*\"peter\" + 0.006*\"turnbul\" + 0.006*\"great\" + 0.006*\"univers\" + 0.005*\"right\" + 0.005*\"beach\" + 0.005*\"human\"\n",
      "Topic: 1 Word: 0.018*\"covid\" + 0.015*\"coronavirus\" + 0.011*\"case\" + 0.011*\"news\" + 0.009*\"victoria\" + 0.009*\"market\" + 0.009*\"record\" + 0.009*\"street\" + 0.009*\"crash\" + 0.007*\"wall\"\n",
      "Topic: 2 Word: 0.017*\"polic\" + 0.016*\"charg\" + 0.015*\"murder\" + 0.012*\"alleg\" + 0.012*\"court\" + 0.010*\"woman\" + 0.009*\"death\" + 0.009*\"restrict\" + 0.009*\"jail\" + 0.008*\"sentenc\"\n",
      "Topic: 3 Word: 0.018*\"vaccin\" + 0.013*\"lockdown\" + 0.009*\"covid\" + 0.009*\"wednesday\" + 0.008*\"pandem\" + 0.007*\"energi\" + 0.007*\"coronavirus\" + 0.007*\"alan\" + 0.006*\"grandstand\" + 0.006*\"open\"\n",
      "Topic: 4 Word: 0.014*\"elect\" + 0.009*\"andrew\" + 0.009*\"australia\" + 0.008*\"friday\" + 0.008*\"monday\" + 0.008*\"quarantin\" + 0.007*\"financ\" + 0.007*\"biden\" + 0.006*\"parti\" + 0.006*\"korea\"\n",
      "Topic: 5 Word: 0.019*\"donald\" + 0.012*\"interview\" + 0.009*\"updat\" + 0.008*\"john\" + 0.007*\"video\" + 0.007*\"australia\" + 0.007*\"daniel\" + 0.007*\"extend\" + 0.007*\"know\" + 0.006*\"footag\"\n",
      "Topic: 6 Word: 0.010*\"live\" + 0.009*\"coronavirus\" + 0.009*\"kill\" + 0.009*\"protest\" + 0.007*\"border\" + 0.006*\"christma\" + 0.006*\"china\" + 0.006*\"australia\" + 0.006*\"dead\" + 0.006*\"octob\"\n",
      "Topic: 7 Word: 0.029*\"trump\" + 0.011*\"coast\" + 0.011*\"morrison\" + 0.009*\"scott\" + 0.009*\"weather\" + 0.008*\"search\" + 0.008*\"gold\" + 0.007*\"miss\" + 0.007*\"michael\" + 0.007*\"season\"\n",
      "Topic: 8 Word: 0.012*\"royal\" + 0.008*\"commiss\" + 0.007*\"victorian\" + 0.007*\"histori\" + 0.007*\"august\" + 0.006*\"cancer\" + 0.006*\"septemb\" + 0.006*\"explain\" + 0.006*\"novemb\" + 0.006*\"june\"\n",
      "Topic: 9 Word: 0.011*\"govern\" + 0.011*\"rural\" + 0.010*\"health\" + 0.009*\"queensland\" + 0.008*\"drum\" + 0.007*\"news\" + 0.007*\"chang\" + 0.006*\"fund\" + 0.006*\"coronavirus\" + 0.006*\"budget\"\n"
     ]
    }
   ],
   "source": [
    "# training the model using the bow corpus\n",
    "\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ratepay', 'group', 'want', 'compulsori', 'local', 'govt', 'vote']\n",
      "ratepayers group wants compulsory local govt voting\n",
      "\n",
      "Score: 0.6123133897781372\t \n",
      "Topic: 0.019*\"plan\" + 0.017*\"hous\" + 0.015*\"rise\" + 0.014*\"region\" + 0.012*\"council\" + 0.012*\"brisban\" + 0.011*\"interview\" + 0.011*\"farm\" + 0.010*\"resid\" + 0.010*\"budget\"\n",
      "\n",
      "Score: 0.15085595846176147\t \n",
      "Topic: 0.063*\"covid\" + 0.034*\"coronavirus\" + 0.025*\"case\" + 0.023*\"vaccin\" + 0.018*\"chang\" + 0.016*\"peopl\" + 0.014*\"victorian\" + 0.013*\"australia\" + 0.011*\"life\" + 0.011*\"say\"\n",
      "\n",
      "Score: 0.14927779138088226\t \n",
      "Topic: 0.048*\"trump\" + 0.039*\"year\" + 0.034*\"elect\" + 0.012*\"presid\" + 0.012*\"fall\" + 0.012*\"say\" + 0.011*\"labor\" + 0.010*\"liber\" + 0.010*\"vote\" + 0.009*\"parti\"\n",
      "\n",
      "Score: 0.012509193271398544\t \n",
      "Topic: 0.024*\"world\" + 0.021*\"school\" + 0.021*\"market\" + 0.020*\"women\" + 0.017*\"busi\" + 0.016*\"australia\" + 0.014*\"speak\" + 0.013*\"win\" + 0.012*\"final\" + 0.012*\"countri\"\n",
      "\n",
      "Score: 0.012507690116763115\t \n",
      "Topic: 0.043*\"queensland\" + 0.028*\"china\" + 0.022*\"report\" + 0.016*\"north\" + 0.015*\"worker\" + 0.015*\"indigen\" + 0.012*\"premier\" + 0.011*\"street\" + 0.011*\"claim\" + 0.011*\"communiti\"\n",
      "\n",
      "Score: 0.01250738836824894\t \n",
      "Topic: 0.022*\"coast\" + 0.019*\"feder\" + 0.016*\"scott\" + 0.014*\"gold\" + 0.013*\"tasmanian\" + 0.013*\"lose\" + 0.011*\"need\" + 0.011*\"game\" + 0.010*\"concern\" + 0.010*\"polit\"\n",
      "\n",
      "Score: 0.012507261708378792\t \n",
      "Topic: 0.054*\"australian\" + 0.034*\"govern\" + 0.023*\"live\" + 0.023*\"health\" + 0.023*\"news\" + 0.023*\"victoria\" + 0.022*\"nation\" + 0.018*\"adelaid\" + 0.010*\"show\" + 0.010*\"servic\"\n",
      "\n",
      "Score: 0.012507236562669277\t \n",
      "Topic: 0.030*\"melbourn\" + 0.026*\"test\" + 0.019*\"restrict\" + 0.019*\"tasmania\" + 0.017*\"lockdown\" + 0.017*\"perth\" + 0.015*\"coronavirus\" + 0.013*\"royal\" + 0.013*\"care\" + 0.013*\"open\"\n",
      "\n",
      "Score: 0.012507016770541668\t \n",
      "Topic: 0.047*\"polic\" + 0.025*\"death\" + 0.023*\"charg\" + 0.019*\"murder\" + 0.018*\"kill\" + 0.016*\"crash\" + 0.016*\"alleg\" + 0.016*\"attack\" + 0.016*\"woman\" + 0.015*\"border\"\n",
      "\n",
      "Score: 0.012507015839219093\t \n",
      "Topic: 0.029*\"record\" + 0.026*\"donald\" + 0.020*\"warn\" + 0.017*\"coronavirus\" + 0.016*\"island\" + 0.016*\"morrison\" + 0.015*\"time\" + 0.014*\"high\" + 0.014*\"victoria\" + 0.013*\"west\"\n"
     ]
    }
   ],
   "source": [
    "# check how part of the training set is classified\n",
    "# first appearing topic is the one assigned to it\n",
    "print(processed_docs[4310])\n",
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.5040776133537292\t \n",
      "Topic: 0.011*\"govern\" + 0.011*\"rural\" + 0.010*\"health\" + 0.009*\"queensland\" + 0.008*\"drum\" + 0.007*\"news\" + 0.007*\"chang\" + 0.006*\"fund\" + 0.006*\"coronavirus\" + 0.006*\"budget\"\n",
      "\n",
      "Score: 0.39588508009910583\t \n",
      "Topic: 0.016*\"countri\" + 0.011*\"hour\" + 0.008*\"thursday\" + 0.007*\"peter\" + 0.006*\"turnbul\" + 0.006*\"great\" + 0.006*\"univers\" + 0.005*\"right\" + 0.005*\"beach\" + 0.005*\"human\"\n",
      "\n",
      "Score: 0.012506840750575066\t \n",
      "Topic: 0.014*\"elect\" + 0.009*\"andrew\" + 0.009*\"australia\" + 0.008*\"friday\" + 0.008*\"monday\" + 0.008*\"quarantin\" + 0.007*\"financ\" + 0.007*\"biden\" + 0.006*\"parti\" + 0.006*\"korea\"\n",
      "\n",
      "Score: 0.01250490639358759\t \n",
      "Topic: 0.018*\"vaccin\" + 0.013*\"lockdown\" + 0.009*\"covid\" + 0.009*\"wednesday\" + 0.008*\"pandem\" + 0.007*\"energi\" + 0.007*\"coronavirus\" + 0.007*\"alan\" + 0.006*\"grandstand\" + 0.006*\"open\"\n",
      "\n",
      "Score: 0.012504869140684605\t \n",
      "Topic: 0.012*\"royal\" + 0.008*\"commiss\" + 0.007*\"victorian\" + 0.007*\"histori\" + 0.007*\"august\" + 0.006*\"cancer\" + 0.006*\"septemb\" + 0.006*\"explain\" + 0.006*\"novemb\" + 0.006*\"june\"\n",
      "\n",
      "Score: 0.01250484213232994\t \n",
      "Topic: 0.018*\"covid\" + 0.015*\"coronavirus\" + 0.011*\"case\" + 0.011*\"news\" + 0.009*\"victoria\" + 0.009*\"market\" + 0.009*\"record\" + 0.009*\"street\" + 0.009*\"crash\" + 0.007*\"wall\"\n",
      "\n",
      "Score: 0.01250419020652771\t \n",
      "Topic: 0.019*\"donald\" + 0.012*\"interview\" + 0.009*\"updat\" + 0.008*\"john\" + 0.007*\"video\" + 0.007*\"australia\" + 0.007*\"daniel\" + 0.007*\"extend\" + 0.007*\"know\" + 0.006*\"footag\"\n",
      "\n",
      "Score: 0.012504136189818382\t \n",
      "Topic: 0.010*\"live\" + 0.009*\"coronavirus\" + 0.009*\"kill\" + 0.009*\"protest\" + 0.007*\"border\" + 0.006*\"christma\" + 0.006*\"china\" + 0.006*\"australia\" + 0.006*\"dead\" + 0.006*\"octob\"\n",
      "\n",
      "Score: 0.01250376459211111\t \n",
      "Topic: 0.017*\"polic\" + 0.016*\"charg\" + 0.015*\"murder\" + 0.012*\"alleg\" + 0.012*\"court\" + 0.010*\"woman\" + 0.009*\"death\" + 0.009*\"restrict\" + 0.009*\"jail\" + 0.008*\"sentenc\"\n",
      "\n",
      "Score: 0.012503722682595253\t \n",
      "Topic: 0.029*\"trump\" + 0.011*\"coast\" + 0.011*\"morrison\" + 0.009*\"scott\" + 0.009*\"weather\" + 0.008*\"search\" + 0.008*\"gold\" + 0.007*\"miss\" + 0.007*\"michael\" + 0.007*\"season\"\n"
     ]
    }
   ],
   "source": [
    "# evaluate the tfidf version\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.425210565328598\t Topic: 0.048*\"trump\" + 0.039*\"year\" + 0.034*\"elect\" + 0.012*\"presid\" + 0.012*\"fall\"\n",
      "Score: 0.3746684491634369\t Topic: 0.029*\"record\" + 0.026*\"donald\" + 0.020*\"warn\" + 0.017*\"coronavirus\" + 0.016*\"island\"\n",
      "Score: 0.0250252615660429\t Topic: 0.019*\"plan\" + 0.017*\"hous\" + 0.015*\"rise\" + 0.014*\"region\" + 0.012*\"council\"\n",
      "Score: 0.02501756325364113\t Topic: 0.024*\"world\" + 0.021*\"school\" + 0.021*\"market\" + 0.020*\"women\" + 0.017*\"busi\"\n",
      "Score: 0.025016717612743378\t Topic: 0.054*\"australian\" + 0.034*\"govern\" + 0.023*\"live\" + 0.023*\"health\" + 0.023*\"news\"\n",
      "Score: 0.025016402825713158\t Topic: 0.063*\"covid\" + 0.034*\"coronavirus\" + 0.025*\"case\" + 0.023*\"vaccin\" + 0.018*\"chang\"\n",
      "Score: 0.025014767423272133\t Topic: 0.047*\"polic\" + 0.025*\"death\" + 0.023*\"charg\" + 0.019*\"murder\" + 0.018*\"kill\"\n",
      "Score: 0.025012994185090065\t Topic: 0.030*\"melbourn\" + 0.026*\"test\" + 0.019*\"restrict\" + 0.019*\"tasmania\" + 0.017*\"lockdown\"\n",
      "Score: 0.02501094713807106\t Topic: 0.043*\"queensland\" + 0.028*\"china\" + 0.022*\"report\" + 0.016*\"north\" + 0.015*\"worker\"\n",
      "Score: 0.02500632219016552\t Topic: 0.022*\"coast\" + 0.019*\"feder\" + 0.016*\"scott\" + 0.014*\"gold\" + 0.013*\"tasmanian\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'trade an apple for an orange'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
