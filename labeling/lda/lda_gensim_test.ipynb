{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim LDA\n",
    "\n",
    "Adapted from https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file = 'npr.csv'      # the input csv file\n",
    "topic_file = 'topics_'+csv_file\n",
    "\n",
    "data_path = 'data/'+csv_file\n",
    "topic_path = 'topics/'+topic_file\n",
    "\n",
    "df = pd.read_csv(data_path);\n",
    "# df = df[['headline_text']]\n",
    "df = df[['Article']]\n",
    "df['index'] = df.index\n",
    "documents = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11992\n",
      "                                             Article  index\n",
      "0  In the Washington of 2016, even when the polic...      0\n",
      "1    Donald Trump has used Twitter  —   his prefe...      1\n",
      "2    Donald Trump is unabashedly praising Russian...      2\n",
      "3  Updated at 2:50 p. m. ET, Russian President Vl...      3\n",
      "4  From photography, illustration and video, to d...      4\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/bansharee/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "stemmer = SnowballStemmer(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words in third person are changed to first person and verbs in past and future tenses are changed into present\n",
    "# words are reduced to their root form\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['Oklahoma', 'City', 'residents', 'woke', 'early', 'New', 'Year’s', 'Day', 'to', 'a', 'magnitude', '4.', '2', 'quake.', 'Earlier', 'this', 'week,', 'a', 'magnitude', '4.', '3', 'quake', 'struck', 'the', 'same', 'area.', 'The', 'state', 'isn’t', 'historically', 'known', 'for', 'earthquakes,', 'but', 'NPR’s', 'Nell', 'Greenfieldboyce', 'told', 'our', 'Newscast', 'unit', 'that', 'Oklahoma', '”has', 'recently', 'seen', 'a', 'dramatic', 'rise', 'in', 'seismic', 'activity.”', 'Here’s', 'more:', '”If', 'you', 'think', 'of', 'a', 'U.', 'S.', 'state', 'associated', 'with', 'earthquakes,', 'it’s', 'probably', 'California.', 'But', 'really,', 'you', 'should', 'think', 'Oklahoma.', 'In', '2015,', 'Oklahoma', 'hit', 'an', '', '', 'high,', 'with', 'more', 'than', '800', 'quakes', 'of', 'magnitude', '3', 'or', 'greater.', 'That', 'busts', 'the', 'record', 'set', 'in', '2014,', 'which', 'topped', 'the', 'previous', 'record', 'set', 'the', 'year', 'before.', 'State', 'officials', 'have', 'said', 'this', 'rise', 'is', 'very', 'unlikely', 'to', 'represent', 'a', 'naturally', 'occurring', 'process.', 'The', 'concern', 'is', 'that', 'these', 'quakes', 'may', 'be', 'linked', 'to', 'oil', 'and', 'gas', 'drilling', '', '—', '', '', 'specifically,', 'the', 'way', 'wastewater', 'produced', 'by', 'the', 'drilling', 'is', 'pumped', 'into', 'deep', 'underground', 'disposal', 'wells.', 'Oklahoma', 'is', 'trying', 'to', 'address', 'the', 'issue.', 'It', 'has', 'a', 'coordinating', 'council', 'on', 'seismic', 'activity', 'that', 'includes', 'regulators,', 'scientists', 'and', 'industry', 'representatives.”', 'Joe', 'Wertz', 'of', 'StateImpact', 'Oklahoma', 'further', 'explained', 'the', 'connection', 'between', 'the', 'oil', 'and', 'gas', 'industry', 'and', 'the', 'increasing', 'number', 'of', 'quakes,', 'on', 'Weekend', 'Edition', 'Saturday', 'in', 'November:', '”Oil', 'and', 'gas', 'production', 'creates', 'a', 'lot', 'of', 'toxic', 'wastewater.', 'To', 'keep', 'it', 'from', 'contaminating', 'drinking', 'water,', 'oil', 'companies', 'inject', 'the', 'fluid', 'into', 'underground', 'disposal', 'wells.', 'That', 'can', 'put', 'pressure', 'on', 'faults,', 'causing', 'them', 'to', 'slip,', 'which', 'scientists', 'say', 'is', 'responsible', 'for', 'Oklahoma’s', 'massive', 'earthquake', 'spike.”', 'And', 'even', 'as', 'scientists', 'blame', 'the', 'oil', 'and', 'gas', 'industry', 'for', 'the', 'new', 'seismic', 'activity,', '”researchers', 'say', 'the', 'earthquakes', 'could', 'compromise', 'the', 'economically', 'vital', 'energy', 'hub.”', 'StateImpact', 'says', 'the', 'National', 'Research', 'Council', 'advises', '”investigating', 'any', 'potential', 'site’s', 'history', 'of', 'earthquakes', 'and', 'its', 'proximity', 'to', 'fault', 'lines.”', 'Other', 'scientists', 'suggest', '”that', 'companies', 'look', 'for', 'new', 'ways', 'of', 'disposing', 'of', 'wastewater', 'altogether.”']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['oklahoma', 'citi', 'resid', 'wake', 'earli', 'year', 'magnitud', 'quak', 'earlier', 'week', 'magnitud', 'quak', 'strike', 'area', 'state', 'histor', 'know', 'earthquak', 'nell', 'greenfieldboyc', 'tell', 'newscast', 'unit', 'oklahoma', 'recent', 'see', 'dramat', 'rise', 'seismic', 'activ', 'think', 'state', 'associ', 'earthquak', 'probabl', 'california', 'think', 'oklahoma', 'oklahoma', 'high', 'quak', 'magnitud', 'greater', 'bust', 'record', 'top', 'previous', 'record', 'year', 'state', 'offici', 'say', 'rise', 'unlik', 'repres', 'natur', 'occur', 'process', 'concern', 'quak', 'link', 'drill', 'specif', 'wastewat', 'produc', 'drill', 'pump', 'deep', 'underground', 'dispos', 'well', 'oklahoma', 'tri', 'address', 'issu', 'coordin', 'council', 'seismic', 'activ', 'includ', 'regul', 'scientist', 'industri', 'repres', 'wertz', 'stateimpact', 'oklahoma', 'explain', 'connect', 'industri', 'increas', 'number', 'quak', 'weekend', 'edit', 'saturday', 'novemb', 'product', 'creat', 'toxic', 'wastewat', 'contamin', 'drink', 'water', 'compani', 'inject', 'fluid', 'underground', 'dispos', 'well', 'pressur', 'fault', 'caus', 'slip', 'scientist', 'respons', 'oklahoma', 'massiv', 'earthquak', 'spike', 'scientist', 'blame', 'industri', 'seismic', 'activ', 'research', 'earthquak', 'compromis', 'econom', 'vital', 'energi', 'stateimpact', 'say', 'nation', 'research', 'council', 'advis', 'investig', 'potenti', 'site', 'histori', 'earthquak', 'proxim', 'fault', 'line', 'scientist', 'suggest', 'compani', 'look', 'way', 'dispos', 'wastewat', 'altogeth']\n"
     ]
    }
   ],
   "source": [
    "# displaying how preprocessing works\n",
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [washington, polici, bipartisan, polit, sens, ...\n",
       "1    [donald, trump, twitter, prefer, mean, communi...\n",
       "2    [donald, trump, unabash, prais, russian, presi...\n",
       "3    [updat, russian, presid, vladimir, putin, say,...\n",
       "4    [photographi, illustr, video, data, visual, im...\n",
       "5    [want, join, yoga, class, hat, beatif, instruc...\n",
       "6    [public, support, debunk, claim, vaccin, caus,...\n",
       "7    [stand, airport, exit, debat, snack, young, ro...\n",
       "8    [movi, tri, realist, summon, batman, shouldn, ...\n",
       "9    [eighteen, year, year, david, fisher, visit, f...\n",
       "Name: Article, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess 'headline_text' text from training set\n",
    "# processed_docs = documents['headline_text'].map(preprocess)\n",
    "processed_docs = documents['Article'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 abil\n",
      "1 accept\n",
      "2 account\n",
      "3 act\n",
      "4 action\n",
      "5 actual\n",
      "6 add\n",
      "7 administr\n",
      "8 advis\n",
      "9 affair\n",
      "10 afloat\n"
     ]
    }
   ],
   "source": [
    "# creating dictionary using words in the training set, mapped to how many times the word appears in the set\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering out:\n",
    "#   * less than 15 documents (absolute number) or\n",
    "#   * more than 0.5 documents (fraction of total corpus size, not absolute number)\n",
    "#   * after the above two steps, keep only the first 100000 most frequent tokens\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "# bow_corpus[0]\n",
    "\n",
    "# bow_corpus = []\n",
    "# for doc in processed_docs:\n",
    "#     bow = dictionary.doc2bow(doc)\n",
    "#     bow_corpus.append(bow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(\n",
    "                                                    bow_doc_4310[i][0], \n",
    "                                                    dictionary[bow_doc_4310[i][0]], \n",
    "                                                    bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF weights words based on how often they appear in a document \n",
    "# versus how often they appear in the entire corpus\n",
    "# this helps LDA distinguish topics by weighting more important words higher\n",
    "\n",
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "\n",
    "# preview of how this works\n",
    "pprint(corpus_tfidf[0])\n",
    "for i in range(len(bow_corpus[0])):\n",
    "    print(dictionary[bow_corpus[0][i][0]], corpus_tfidf[0][i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: HDP goes here for num_topics\n",
    "# see https://medium.com/analytics-vidhya/text-classification-using-lda-35d5b98d4f05 for HDP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Words: 0.005*\"want\" + 0.005*\"thing\" + 0.004*\"feel\" + 0.004*\"live\" + 0.004*\"write\" + 0.004*\"book\" + 0.004*\"stori\" + 0.003*\"women\" + 0.003*\"world\" + 0.003*\"right\"\n",
      "Topic: 1 Words: 0.006*\"food\" + 0.004*\"famili\" + 0.004*\"want\" + 0.004*\"look\" + 0.003*\"live\" + 0.003*\"need\" + 0.003*\"start\" + 0.003*\"help\" + 0.003*\"call\" + 0.003*\"thing\"\n",
      "Topic: 2 Words: 0.019*\"trump\" + 0.006*\"clinton\" + 0.006*\"presid\" + 0.005*\"campaign\" + 0.005*\"polit\" + 0.005*\"state\" + 0.004*\"news\" + 0.004*\"report\" + 0.004*\"countri\" + 0.004*\"nation\"\n",
      "Topic: 3 Words: 0.012*\"health\" + 0.007*\"care\" + 0.006*\"state\" + 0.005*\"patient\" + 0.005*\"insur\" + 0.004*\"medic\" + 0.004*\"research\" + 0.004*\"plan\" + 0.004*\"hospit\" + 0.004*\"need\"\n",
      "Topic: 4 Words: 0.007*\"report\" + 0.005*\"attack\" + 0.004*\"state\" + 0.003*\"countri\" + 0.003*\"call\" + 0.003*\"forc\" + 0.003*\"polic\" + 0.003*\"kill\" + 0.003*\"live\" + 0.003*\"group\"\n",
      "Topic: 5 Words: 0.012*\"school\" + 0.009*\"student\" + 0.006*\"state\" + 0.004*\"educ\" + 0.004*\"report\" + 0.004*\"famili\" + 0.003*\"help\" + 0.003*\"american\" + 0.003*\"children\" + 0.003*\"countri\"\n",
      "Topic: 6 Words: 0.008*\"polic\" + 0.008*\"report\" + 0.005*\"state\" + 0.005*\"offic\" + 0.004*\"citi\" + 0.004*\"trump\" + 0.004*\"women\" + 0.003*\"take\" + 0.003*\"presid\" + 0.003*\"shoot\"\n",
      "Topic: 7 Words: 0.010*\"trump\" + 0.006*\"percent\" + 0.006*\"report\" + 0.005*\"state\" + 0.005*\"countri\" + 0.005*\"studi\" + 0.005*\"presid\" + 0.003*\"research\" + 0.003*\"american\" + 0.003*\"chang\"\n",
      "Topic: 8 Words: 0.013*\"trump\" + 0.007*\"clinton\" + 0.006*\"state\" + 0.006*\"presid\" + 0.006*\"vote\" + 0.006*\"court\" + 0.005*\"republican\" + 0.004*\"elect\" + 0.004*\"hous\" + 0.004*\"democrat\"\n",
      "Topic: 9 Words: 0.007*\"trump\" + 0.007*\"music\" + 0.004*\"state\" + 0.004*\"clinton\" + 0.004*\"song\" + 0.003*\"album\" + 0.003*\"play\" + 0.003*\"record\" + 0.003*\"report\" + 0.003*\"take\"\n"
     ]
    }
   ],
   "source": [
    "# training the model using the bow corpus\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} Words: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.004*\"trump\" + 0.002*\"clinton\" + 0.001*\"presid\" + 0.001*\"polic\" + 0.001*\"report\" + 0.001*\"elect\" + 0.001*\"women\" + 0.001*\"state\" + 0.001*\"campaign\" + 0.001*\"student\"\n",
      "Topic: 1 Word: 0.004*\"zika\" + 0.003*\"trump\" + 0.002*\"virus\" + 0.002*\"mosquito\" + 0.002*\"climat\" + 0.001*\"hous\" + 0.001*\"women\" + 0.001*\"health\" + 0.001*\"presid\" + 0.001*\"clinton\"\n",
      "Topic: 2 Word: 0.002*\"music\" + 0.002*\"song\" + 0.002*\"album\" + 0.001*\"health\" + 0.001*\"parent\" + 0.001*\"school\" + 0.001*\"trump\" + 0.001*\"artist\" + 0.001*\"dylan\" + 0.001*\"band\"\n",
      "Topic: 3 Word: 0.003*\"trump\" + 0.002*\"polic\" + 0.001*\"isra\" + 0.001*\"clinton\" + 0.001*\"israel\" + 0.001*\"state\" + 0.001*\"palestinian\" + 0.001*\"report\" + 0.001*\"presid\" + 0.001*\"attack\"\n",
      "Topic: 4 Word: 0.003*\"refuge\" + 0.002*\"olymp\" + 0.002*\"food\" + 0.002*\"school\" + 0.001*\"student\" + 0.001*\"song\" + 0.001*\"trump\" + 0.001*\"game\" + 0.001*\"water\" + 0.001*\"women\"\n",
      "Topic: 5 Word: 0.003*\"trump\" + 0.002*\"dutert\" + 0.002*\"drug\" + 0.002*\"philippin\" + 0.001*\"clinton\" + 0.001*\"vote\" + 0.001*\"yahoo\" + 0.001*\"presid\" + 0.001*\"pope\" + 0.001*\"china\"\n",
      "Topic: 6 Word: 0.006*\"health\" + 0.005*\"insur\" + 0.003*\"care\" + 0.003*\"trump\" + 0.003*\"patient\" + 0.002*\"abort\" + 0.002*\"coverag\" + 0.002*\"plan\" + 0.002*\"medicaid\" + 0.002*\"obamacar\"\n",
      "Topic: 7 Word: 0.002*\"health\" + 0.002*\"food\" + 0.002*\"studi\" + 0.002*\"patient\" + 0.002*\"diseas\" + 0.001*\"music\" + 0.001*\"opioid\" + 0.001*\"drug\" + 0.001*\"percent\" + 0.001*\"medic\"\n",
      "Topic: 8 Word: 0.005*\"trump\" + 0.004*\"clinton\" + 0.003*\"vote\" + 0.003*\"democrat\" + 0.002*\"republican\" + 0.002*\"sander\" + 0.002*\"percent\" + 0.002*\"voter\" + 0.002*\"state\" + 0.002*\"elector\"\n",
      "Topic: 9 Word: 0.002*\"trump\" + 0.002*\"drug\" + 0.002*\"vaccin\" + 0.001*\"health\" + 0.001*\"food\" + 0.001*\"school\" + 0.001*\"women\" + 0.001*\"book\" + 0.001*\"polic\" + 0.001*\"homeless\"\n"
     ]
    }
   ],
   "source": [
    "# training the model using the bow corpus\n",
    "\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oklahoma', 'citi', 'resid', 'wake', 'earli', 'year', 'magnitud', 'quak', 'earlier', 'week', 'magnitud', 'quak', 'strike', 'area', 'state', 'histor', 'know', 'earthquak', 'nell', 'greenfieldboyc', 'tell', 'newscast', 'unit', 'oklahoma', 'recent', 'see', 'dramat', 'rise', 'seismic', 'activ', 'think', 'state', 'associ', 'earthquak', 'probabl', 'california', 'think', 'oklahoma', 'oklahoma', 'high', 'quak', 'magnitud', 'greater', 'bust', 'record', 'top', 'previous', 'record', 'year', 'state', 'offici', 'say', 'rise', 'unlik', 'repres', 'natur', 'occur', 'process', 'concern', 'quak', 'link', 'drill', 'specif', 'wastewat', 'produc', 'drill', 'pump', 'deep', 'underground', 'dispos', 'well', 'oklahoma', 'tri', 'address', 'issu', 'coordin', 'council', 'seismic', 'activ', 'includ', 'regul', 'scientist', 'industri', 'repres', 'wertz', 'stateimpact', 'oklahoma', 'explain', 'connect', 'industri', 'increas', 'number', 'quak', 'weekend', 'edit', 'saturday', 'novemb', 'product', 'creat', 'toxic', 'wastewat', 'contamin', 'drink', 'water', 'compani', 'inject', 'fluid', 'underground', 'dispos', 'well', 'pressur', 'fault', 'caus', 'slip', 'scientist', 'respons', 'oklahoma', 'massiv', 'earthquak', 'spike', 'scientist', 'blame', 'industri', 'seismic', 'activ', 'research', 'earthquak', 'compromis', 'econom', 'vital', 'energi', 'stateimpact', 'say', 'nation', 'research', 'council', 'advis', 'investig', 'potenti', 'site', 'histori', 'earthquak', 'proxim', 'fault', 'line', 'scientist', 'suggest', 'compani', 'look', 'way', 'dispos', 'wastewat', 'altogeth']\n",
      "\n",
      "Score: 0.6216321587562561\t \n",
      "Topic: 0.008*\"polic\" + 0.008*\"report\" + 0.005*\"state\" + 0.005*\"offic\" + 0.004*\"citi\"\n",
      "\n",
      "Score: 0.37260526418685913\t \n",
      "Topic: 0.010*\"trump\" + 0.006*\"percent\" + 0.006*\"report\" + 0.005*\"state\" + 0.005*\"countri\"\n"
     ]
    }
   ],
   "source": [
    "# check how part of the training set is classified\n",
    "# first appearing topic is the one assigned to it\n",
    "print(processed_docs[4310])\n",
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.993514358997345\t \n",
      "Topic: 0.004*\"trump\" + 0.002*\"clinton\" + 0.001*\"presid\" + 0.001*\"polic\" + 0.001*\"report\"\n"
     ]
    }
   ],
   "source": [
    "# evaluate the tfidf version\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.699625551700592\t Topic: 0.005*\"want\" + 0.005*\"thing\" + 0.004*\"feel\" + 0.004*\"live\" + 0.004*\"write\"\n",
      "Score: 0.03337695449590683\t Topic: 0.007*\"trump\" + 0.007*\"music\" + 0.004*\"state\" + 0.004*\"clinton\" + 0.004*\"song\"\n",
      "Score: 0.03337628021836281\t Topic: 0.010*\"trump\" + 0.006*\"percent\" + 0.006*\"report\" + 0.005*\"state\" + 0.005*\"countri\"\n",
      "Score: 0.03337552398443222\t Topic: 0.012*\"health\" + 0.007*\"care\" + 0.006*\"state\" + 0.005*\"patient\" + 0.005*\"insur\"\n",
      "Score: 0.03337501734495163\t Topic: 0.019*\"trump\" + 0.006*\"clinton\" + 0.006*\"presid\" + 0.005*\"campaign\" + 0.005*\"polit\"\n",
      "Score: 0.03337496146559715\t Topic: 0.008*\"polic\" + 0.008*\"report\" + 0.005*\"state\" + 0.005*\"offic\" + 0.004*\"citi\"\n",
      "Score: 0.033374760299921036\t Topic: 0.007*\"report\" + 0.005*\"attack\" + 0.004*\"state\" + 0.003*\"countri\" + 0.003*\"call\"\n",
      "Score: 0.03337417542934418\t Topic: 0.013*\"trump\" + 0.007*\"clinton\" + 0.006*\"state\" + 0.006*\"presid\" + 0.006*\"vote\"\n",
      "Score: 0.033373814076185226\t Topic: 0.012*\"school\" + 0.009*\"student\" + 0.006*\"state\" + 0.004*\"educ\" + 0.004*\"report\"\n",
      "Score: 0.033372990787029266\t Topic: 0.006*\"food\" + 0.004*\"famili\" + 0.004*\"want\" + 0.004*\"look\" + 0.003*\"live\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'writing out the equation for Euler'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
