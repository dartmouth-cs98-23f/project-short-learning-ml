{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim LDA\n",
    "\n",
    "Adapted from https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file = 'npr.csv'      # the input csv file\n",
    "topic_file = 'topics_'+csv_file\n",
    "\n",
    "data_path = 'data/'+csv_file\n",
    "topic_path = 'topics/'+topic_file\n",
    "\n",
    "df = pd.read_csv(data_path);\n",
    "# df = df[['headline_text']]\n",
    "df = df[['Article']]\n",
    "df['index'] = df.index\n",
    "documents = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11992\n",
      "                                             Article  index\n",
      "0  In the Washington of 2016, even when the polic...      0\n",
      "1    Donald Trump has used Twitter  —   his prefe...      1\n",
      "2    Donald Trump is unabashedly praising Russian...      2\n",
      "3  Updated at 2:50 p. m. ET, Russian President Vl...      3\n",
      "4  From photography, illustration and video, to d...      4\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/bansharee/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "stemmer = SnowballStemmer(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words in third person are changed to first person and verbs in past and future tenses are changed into present\n",
    "# words are reduced to their root form\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['Oklahoma', 'City', 'residents', 'woke', 'early', 'New', 'Year’s', 'Day', 'to', 'a', 'magnitude', '4.', '2', 'quake.', 'Earlier', 'this', 'week,', 'a', 'magnitude', '4.', '3', 'quake', 'struck', 'the', 'same', 'area.', 'The', 'state', 'isn’t', 'historically', 'known', 'for', 'earthquakes,', 'but', 'NPR’s', 'Nell', 'Greenfieldboyce', 'told', 'our', 'Newscast', 'unit', 'that', 'Oklahoma', '”has', 'recently', 'seen', 'a', 'dramatic', 'rise', 'in', 'seismic', 'activity.”', 'Here’s', 'more:', '”If', 'you', 'think', 'of', 'a', 'U.', 'S.', 'state', 'associated', 'with', 'earthquakes,', 'it’s', 'probably', 'California.', 'But', 'really,', 'you', 'should', 'think', 'Oklahoma.', 'In', '2015,', 'Oklahoma', 'hit', 'an', '', '', 'high,', 'with', 'more', 'than', '800', 'quakes', 'of', 'magnitude', '3', 'or', 'greater.', 'That', 'busts', 'the', 'record', 'set', 'in', '2014,', 'which', 'topped', 'the', 'previous', 'record', 'set', 'the', 'year', 'before.', 'State', 'officials', 'have', 'said', 'this', 'rise', 'is', 'very', 'unlikely', 'to', 'represent', 'a', 'naturally', 'occurring', 'process.', 'The', 'concern', 'is', 'that', 'these', 'quakes', 'may', 'be', 'linked', 'to', 'oil', 'and', 'gas', 'drilling', '', '—', '', '', 'specifically,', 'the', 'way', 'wastewater', 'produced', 'by', 'the', 'drilling', 'is', 'pumped', 'into', 'deep', 'underground', 'disposal', 'wells.', 'Oklahoma', 'is', 'trying', 'to', 'address', 'the', 'issue.', 'It', 'has', 'a', 'coordinating', 'council', 'on', 'seismic', 'activity', 'that', 'includes', 'regulators,', 'scientists', 'and', 'industry', 'representatives.”', 'Joe', 'Wertz', 'of', 'StateImpact', 'Oklahoma', 'further', 'explained', 'the', 'connection', 'between', 'the', 'oil', 'and', 'gas', 'industry', 'and', 'the', 'increasing', 'number', 'of', 'quakes,', 'on', 'Weekend', 'Edition', 'Saturday', 'in', 'November:', '”Oil', 'and', 'gas', 'production', 'creates', 'a', 'lot', 'of', 'toxic', 'wastewater.', 'To', 'keep', 'it', 'from', 'contaminating', 'drinking', 'water,', 'oil', 'companies', 'inject', 'the', 'fluid', 'into', 'underground', 'disposal', 'wells.', 'That', 'can', 'put', 'pressure', 'on', 'faults,', 'causing', 'them', 'to', 'slip,', 'which', 'scientists', 'say', 'is', 'responsible', 'for', 'Oklahoma’s', 'massive', 'earthquake', 'spike.”', 'And', 'even', 'as', 'scientists', 'blame', 'the', 'oil', 'and', 'gas', 'industry', 'for', 'the', 'new', 'seismic', 'activity,', '”researchers', 'say', 'the', 'earthquakes', 'could', 'compromise', 'the', 'economically', 'vital', 'energy', 'hub.”', 'StateImpact', 'says', 'the', 'National', 'Research', 'Council', 'advises', '”investigating', 'any', 'potential', 'site’s', 'history', 'of', 'earthquakes', 'and', 'its', 'proximity', 'to', 'fault', 'lines.”', 'Other', 'scientists', 'suggest', '”that', 'companies', 'look', 'for', 'new', 'ways', 'of', 'disposing', 'of', 'wastewater', 'altogether.”']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['oklahoma', 'citi', 'resid', 'wake', 'earli', 'year', 'magnitud', 'quak', 'earlier', 'week', 'magnitud', 'quak', 'strike', 'area', 'state', 'histor', 'know', 'earthquak', 'nell', 'greenfieldboyc', 'tell', 'newscast', 'unit', 'oklahoma', 'recent', 'see', 'dramat', 'rise', 'seismic', 'activ', 'think', 'state', 'associ', 'earthquak', 'probabl', 'california', 'think', 'oklahoma', 'oklahoma', 'high', 'quak', 'magnitud', 'greater', 'bust', 'record', 'top', 'previous', 'record', 'year', 'state', 'offici', 'say', 'rise', 'unlik', 'repres', 'natur', 'occur', 'process', 'concern', 'quak', 'link', 'drill', 'specif', 'wastewat', 'produc', 'drill', 'pump', 'deep', 'underground', 'dispos', 'well', 'oklahoma', 'tri', 'address', 'issu', 'coordin', 'council', 'seismic', 'activ', 'includ', 'regul', 'scientist', 'industri', 'repres', 'wertz', 'stateimpact', 'oklahoma', 'explain', 'connect', 'industri', 'increas', 'number', 'quak', 'weekend', 'edit', 'saturday', 'novemb', 'product', 'creat', 'toxic', 'wastewat', 'contamin', 'drink', 'water', 'compani', 'inject', 'fluid', 'underground', 'dispos', 'well', 'pressur', 'fault', 'caus', 'slip', 'scientist', 'respons', 'oklahoma', 'massiv', 'earthquak', 'spike', 'scientist', 'blame', 'industri', 'seismic', 'activ', 'research', 'earthquak', 'compromis', 'econom', 'vital', 'energi', 'stateimpact', 'say', 'nation', 'research', 'council', 'advis', 'investig', 'potenti', 'site', 'histori', 'earthquak', 'proxim', 'fault', 'line', 'scientist', 'suggest', 'compani', 'look', 'way', 'dispos', 'wastewat', 'altogeth']\n"
     ]
    }
   ],
   "source": [
    "# displaying how preprocessing works\n",
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [washington, polici, bipartisan, polit, sens, ...\n",
       "1    [donald, trump, twitter, prefer, mean, communi...\n",
       "2    [donald, trump, unabash, prais, russian, presi...\n",
       "3    [updat, russian, presid, vladimir, putin, say,...\n",
       "4    [photographi, illustr, video, data, visual, im...\n",
       "5    [want, join, yoga, class, hat, beatif, instruc...\n",
       "6    [public, support, debunk, claim, vaccin, caus,...\n",
       "7    [stand, airport, exit, debat, snack, young, ro...\n",
       "8    [movi, tri, realist, summon, batman, shouldn, ...\n",
       "9    [eighteen, year, year, david, fisher, visit, f...\n",
       "Name: Article, dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess 'headline_text' text from training set\n",
    "# processed_docs = documents['headline_text'].map(preprocess)\n",
    "processed_docs = documents['Article'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 abil\n",
      "1 accept\n",
      "2 account\n",
      "3 act\n",
      "4 action\n",
      "5 actual\n",
      "6 add\n",
      "7 administr\n",
      "8 advis\n",
      "9 affair\n",
      "10 afloat\n"
     ]
    }
   ],
   "source": [
    "# creating dictionary using words in the training set, mapped to how many times the word appears in the set\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering out:\n",
    "#   * less than 15 documents (absolute number) or\n",
    "#   * more than 0.5 documents (fraction of total corpus size, not absolute number)\n",
    "#   * after the above two steps, keep only the first 100000 most frequent tokens\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 4),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 3),\n",
       " (12, 1),\n",
       " (13, 2),\n",
       " (14, 1),\n",
       " (15, 2),\n",
       " (16, 2),\n",
       " (17, 1),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (20, 2),\n",
       " (21, 1),\n",
       " (22, 3),\n",
       " (23, 1),\n",
       " (24, 1),\n",
       " (25, 1),\n",
       " (26, 2),\n",
       " (27, 2),\n",
       " (28, 1),\n",
       " (29, 1),\n",
       " (30, 1),\n",
       " (31, 1),\n",
       " (32, 1),\n",
       " (33, 1),\n",
       " (34, 1),\n",
       " (35, 1),\n",
       " (36, 1),\n",
       " (37, 1),\n",
       " (38, 2),\n",
       " (39, 1),\n",
       " (40, 1),\n",
       " (41, 1),\n",
       " (42, 1),\n",
       " (43, 7),\n",
       " (44, 4),\n",
       " (45, 1),\n",
       " (46, 1),\n",
       " (47, 2),\n",
       " (48, 1),\n",
       " (49, 1),\n",
       " (50, 1),\n",
       " (51, 1),\n",
       " (52, 2),\n",
       " (53, 1),\n",
       " (54, 1),\n",
       " (55, 1),\n",
       " (56, 1),\n",
       " (57, 5),\n",
       " (58, 1),\n",
       " (59, 1),\n",
       " (60, 1),\n",
       " (61, 1),\n",
       " (62, 1),\n",
       " (63, 2),\n",
       " (64, 1),\n",
       " (65, 1),\n",
       " (66, 1),\n",
       " (67, 1),\n",
       " (68, 1),\n",
       " (69, 1),\n",
       " (70, 1),\n",
       " (71, 1),\n",
       " (72, 1),\n",
       " (73, 1),\n",
       " (74, 1),\n",
       " (75, 1),\n",
       " (76, 1),\n",
       " (77, 2),\n",
       " (78, 1),\n",
       " (79, 1),\n",
       " (80, 1),\n",
       " (81, 1),\n",
       " (82, 1),\n",
       " (83, 1),\n",
       " (84, 1),\n",
       " (85, 1),\n",
       " (86, 1),\n",
       " (87, 1),\n",
       " (88, 1),\n",
       " (89, 1),\n",
       " (90, 1),\n",
       " (91, 1),\n",
       " (92, 1),\n",
       " (93, 1),\n",
       " (94, 1),\n",
       " (95, 2),\n",
       " (96, 1),\n",
       " (97, 1),\n",
       " (98, 2),\n",
       " (99, 1),\n",
       " (100, 1),\n",
       " (101, 4),\n",
       " (102, 1),\n",
       " (103, 1),\n",
       " (104, 3),\n",
       " (105, 1),\n",
       " (106, 1),\n",
       " (107, 5),\n",
       " (108, 1),\n",
       " (109, 1),\n",
       " (110, 1),\n",
       " (111, 1),\n",
       " (112, 1),\n",
       " (113, 1),\n",
       " (114, 1),\n",
       " (115, 1),\n",
       " (116, 1),\n",
       " (117, 1),\n",
       " (118, 3),\n",
       " (119, 1),\n",
       " (120, 1),\n",
       " (121, 1),\n",
       " (122, 1),\n",
       " (123, 1),\n",
       " (124, 1),\n",
       " (125, 1),\n",
       " (126, 1),\n",
       " (127, 4),\n",
       " (128, 2),\n",
       " (129, 1),\n",
       " (130, 1),\n",
       " (131, 1),\n",
       " (132, 1),\n",
       " (133, 1),\n",
       " (134, 1),\n",
       " (135, 1),\n",
       " (136, 1),\n",
       " (137, 1),\n",
       " (138, 1),\n",
       " (139, 2),\n",
       " (140, 1),\n",
       " (141, 1),\n",
       " (142, 1),\n",
       " (143, 4),\n",
       " (144, 2),\n",
       " (145, 2),\n",
       " (146, 1),\n",
       " (147, 1),\n",
       " (148, 1),\n",
       " (149, 1),\n",
       " (150, 1),\n",
       " (151, 1),\n",
       " (152, 1),\n",
       " (153, 1),\n",
       " (154, 3),\n",
       " (155, 1),\n",
       " (156, 1),\n",
       " (157, 1),\n",
       " (158, 4),\n",
       " (159, 1),\n",
       " (160, 1),\n",
       " (161, 9),\n",
       " (162, 6),\n",
       " (163, 1),\n",
       " (164, 1),\n",
       " (165, 2),\n",
       " (166, 4),\n",
       " (167, 1),\n",
       " (168, 1),\n",
       " (169, 2),\n",
       " (170, 4),\n",
       " (171, 2),\n",
       " (172, 1),\n",
       " (173, 1),\n",
       " (174, 1),\n",
       " (175, 3),\n",
       " (176, 1),\n",
       " (177, 1),\n",
       " (178, 1),\n",
       " (179, 3),\n",
       " (180, 3),\n",
       " (181, 2),\n",
       " (182, 1),\n",
       " (183, 1),\n",
       " (184, 4),\n",
       " (185, 1),\n",
       " (186, 2),\n",
       " (187, 1),\n",
       " (188, 1),\n",
       " (189, 1),\n",
       " (190, 1),\n",
       " (191, 2),\n",
       " (192, 1),\n",
       " (193, 2),\n",
       " (194, 2),\n",
       " (195, 2),\n",
       " (196, 3),\n",
       " (197, 1),\n",
       " (198, 1),\n",
       " (199, 1),\n",
       " (200, 1),\n",
       " (201, 15),\n",
       " (202, 3),\n",
       " (203, 1),\n",
       " (204, 1),\n",
       " (205, 2),\n",
       " (206, 1),\n",
       " (207, 1),\n",
       " (208, 1),\n",
       " (209, 1),\n",
       " (210, 1),\n",
       " (211, 1),\n",
       " (212, 1),\n",
       " (213, 1),\n",
       " (214, 1),\n",
       " (215, 1),\n",
       " (216, 1),\n",
       " (217, 1),\n",
       " (218, 2),\n",
       " (219, 1),\n",
       " (220, 1),\n",
       " (221, 1),\n",
       " (222, 1),\n",
       " (223, 1),\n",
       " (224, 1),\n",
       " (225, 1),\n",
       " (226, 1),\n",
       " (227, 2),\n",
       " (228, 3),\n",
       " (229, 1),\n",
       " (230, 1),\n",
       " (231, 1),\n",
       " (232, 8),\n",
       " (233, 1),\n",
       " (234, 1),\n",
       " (235, 1),\n",
       " (236, 1),\n",
       " (237, 1),\n",
       " (238, 1),\n",
       " (239, 1),\n",
       " (240, 2),\n",
       " (241, 1),\n",
       " (242, 1),\n",
       " (243, 7),\n",
       " (244, 1),\n",
       " (245, 1),\n",
       " (246, 1),\n",
       " (247, 2),\n",
       " (248, 1),\n",
       " (249, 1),\n",
       " (250, 1),\n",
       " (251, 1),\n",
       " (252, 1),\n",
       " (253, 1),\n",
       " (254, 1),\n",
       " (255, 1),\n",
       " (256, 6),\n",
       " (257, 2),\n",
       " (258, 1),\n",
       " (259, 1),\n",
       " (260, 1),\n",
       " (261, 1),\n",
       " (262, 1),\n",
       " (263, 1),\n",
       " (264, 2),\n",
       " (265, 1),\n",
       " (266, 1),\n",
       " (267, 9),\n",
       " (268, 14),\n",
       " (269, 1),\n",
       " (270, 7),\n",
       " (271, 3),\n",
       " (272, 2),\n",
       " (273, 1),\n",
       " (274, 1),\n",
       " (275, 3),\n",
       " (276, 2),\n",
       " (277, 1),\n",
       " (278, 1),\n",
       " (279, 1),\n",
       " (280, 2),\n",
       " (281, 1),\n",
       " (282, 4),\n",
       " (283, 1),\n",
       " (284, 1),\n",
       " (285, 1),\n",
       " (286, 1),\n",
       " (287, 1),\n",
       " (288, 1),\n",
       " (289, 1),\n",
       " (290, 1),\n",
       " (291, 1),\n",
       " (292, 1),\n",
       " (293, 1),\n",
       " (294, 1),\n",
       " (295, 1),\n",
       " (296, 1),\n",
       " (297, 1),\n",
       " (298, 3),\n",
       " (299, 2),\n",
       " (300, 1),\n",
       " (301, 1),\n",
       " (302, 2),\n",
       " (303, 1),\n",
       " (304, 2),\n",
       " (305, 1),\n",
       " (306, 1),\n",
       " (307, 1),\n",
       " (308, 1),\n",
       " (309, 1),\n",
       " (310, 1),\n",
       " (311, 1),\n",
       " (312, 2),\n",
       " (313, 1),\n",
       " (314, 1),\n",
       " (315, 1),\n",
       " (316, 19),\n",
       " (317, 1),\n",
       " (318, 1),\n",
       " (319, 1),\n",
       " (320, 1),\n",
       " (321, 1),\n",
       " (322, 1),\n",
       " (323, 1),\n",
       " (324, 2),\n",
       " (325, 1),\n",
       " (326, 1),\n",
       " (327, 1),\n",
       " (328, 1),\n",
       " (329, 1),\n",
       " (330, 2),\n",
       " (331, 1),\n",
       " (332, 2),\n",
       " (333, 1),\n",
       " (334, 8),\n",
       " (335, 1),\n",
       " (336, 4),\n",
       " (337, 1),\n",
       " (338, 1)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[0]\n",
    "\n",
    "# bow_corpus = []\n",
    "# for doc in processed_docs:\n",
    "#     bow = dictionary.doc2bow(doc)\n",
    "#     bow_corpus.append(bow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 8 (\"advis\") appears 1 time.\n",
      "Word 51 (\"citi\") appears 1 time.\n",
      "Word 61 (\"compromis\") appears 1 time.\n",
      "Word 63 (\"concern\") appears 1 time.\n",
      "Word 105 (\"econom\") appears 1 time.\n",
      "Word 140 (\"greater\") appears 1 time.\n",
      "Word 158 (\"includ\") appears 1 time.\n",
      "Word 166 (\"issu\") appears 1 time.\n",
      "Word 179 (\"look\") appears 1 time.\n",
      "Word 195 (\"nation\") appears 1 time.\n",
      "Word 206 (\"offici\") appears 1 time.\n",
      "Word 247 (\"recent\") appears 1 time.\n",
      "Word 259 (\"respons\") appears 1 time.\n",
      "Word 298 (\"suggest\") appears 1 time.\n",
      "Word 314 (\"tri\") appears 1 time.\n",
      "Word 323 (\"unit\") appears 1 time.\n",
      "Word 334 (\"week\") appears 1 time.\n",
      "Word 357 (\"compani\") appears 2 time.\n",
      "Word 362 (\"council\") appears 2 time.\n",
      "Word 374 (\"earlier\") appears 1 time.\n",
      "Word 406 (\"line\") appears 1 time.\n",
      "Word 451 (\"state\") appears 3 time.\n",
      "Word 497 (\"earli\") appears 1 time.\n",
      "Word 515 (\"increas\") appears 1 time.\n",
      "Word 569 (\"unlik\") appears 1 time.\n",
      "Word 572 (\"wake\") appears 1 time.\n",
      "Word 615 (\"specif\") appears 1 time.\n",
      "Word 636 (\"area\") appears 1 time.\n",
      "Word 694 (\"creat\") appears 1 time.\n",
      "Word 737 (\"energi\") appears 1 time.\n",
      "Word 754 (\"explain\") appears 1 time.\n",
      "Word 786 (\"histori\") appears 1 time.\n",
      "Word 848 (\"number\") appears 1 time.\n",
      "Word 874 (\"potenti\") appears 1 time.\n",
      "Word 878 (\"previous\") appears 1 time.\n",
      "Word 879 (\"probabl\") appears 1 time.\n",
      "Word 894 (\"record\") appears 2 time.\n",
      "Word 898 (\"regul\") appears 1 time.\n",
      "Word 930 (\"site\") appears 1 time.\n",
      "Word 993 (\"water\") appears 1 time.\n",
      "Word 994 (\"way\") appears 1 time.\n",
      "Word 1004 (\"activ\") appears 3 time.\n",
      "Word 1166 (\"altogeth\") appears 1 time.\n",
      "Word 1188 (\"caus\") appears 1 time.\n",
      "Word 1201 (\"connect\") appears 1 time.\n",
      "Word 1237 (\"fluid\") appears 1 time.\n",
      "Word 1246 (\"high\") appears 1 time.\n",
      "Word 1251 (\"inject\") appears 1 time.\n",
      "Word 1277 (\"natur\") appears 1 time.\n",
      "Word 1285 (\"occur\") appears 1 time.\n",
      "Word 1301 (\"produc\") appears 1 time.\n",
      "Word 1302 (\"product\") appears 1 time.\n",
      "Word 1315 (\"research\") appears 2 time.\n",
      "Word 1322 (\"scientist\") appears 4 time.\n",
      "Word 1337 (\"strike\") appears 1 time.\n",
      "Word 1347 (\"toxic\") appears 1 time.\n",
      "Word 1374 (\"associ\") appears 1 time.\n",
      "Word 1394 (\"drink\") appears 1 time.\n",
      "Word 1420 (\"link\") appears 1 time.\n",
      "Word 1424 (\"magnitud\") appears 3 time.\n",
      "Word 1638 (\"see\") appears 1 time.\n",
      "Word 1662 (\"vital\") appears 1 time.\n",
      "Word 1691 (\"deep\") appears 1 time.\n",
      "Word 1886 (\"spike\") appears 1 time.\n",
      "Word 1910 (\"california\") appears 1 time.\n",
      "Word 1917 (\"dramat\") appears 1 time.\n",
      "Word 1928 (\"histor\") appears 1 time.\n",
      "Word 1955 (\"resid\") appears 1 time.\n",
      "Word 2048 (\"novemb\") appears 1 time.\n",
      "Word 2057 (\"pressur\") appears 1 time.\n",
      "Word 2058 (\"process\") appears 1 time.\n",
      "Word 2147 (\"rise\") appears 2 time.\n",
      "Word 2183 (\"edit\") appears 1 time.\n",
      "Word 2233 (\"weekend\") appears 1 time.\n",
      "Word 2564 (\"well\") appears 2 time.\n",
      "Word 2611 (\"industri\") appears 3 time.\n",
      "Word 2688 (\"newscast\") appears 1 time.\n",
      "Word 2697 (\"saturday\") appears 1 time.\n",
      "Word 2729 (\"investig\") appears 1 time.\n",
      "Word 2900 (\"blame\") appears 1 time.\n",
      "Word 3327 (\"repres\") appears 2 time.\n",
      "Word 3348 (\"slip\") appears 1 time.\n",
      "Word 3474 (\"massiv\") appears 1 time.\n",
      "Word 3662 (\"drill\") appears 2 time.\n",
      "Word 3676 (\"oklahoma\") appears 7 time.\n",
      "Word 3726 (\"address\") appears 1 time.\n",
      "Word 3787 (\"fault\") appears 2 time.\n",
      "Word 3816 (\"underground\") appears 2 time.\n",
      "Word 3988 (\"top\") appears 1 time.\n",
      "Word 4082 (\"contamin\") appears 1 time.\n",
      "Word 4502 (\"coordin\") appears 1 time.\n",
      "Word 4515 (\"proxim\") appears 1 time.\n",
      "Word 4581 (\"dispos\") appears 3 time.\n",
      "Word 5644 (\"bust\") appears 1 time.\n",
      "Word 6681 (\"pump\") appears 1 time.\n",
      "Word 7175 (\"earthquak\") appears 5 time.\n",
      "Word 7938 (\"nell\") appears 1 time.\n",
      "Word 9145 (\"seismic\") appears 3 time.\n",
      "Word 10888 (\"quak\") appears 5 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(\n",
    "                                                    bow_doc_4310[i][0], \n",
    "                                                    dictionary[bow_doc_4310[i][0]], \n",
    "                                                    bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.023067990364550667),\n",
      " (1, 0.02138290697692375),\n",
      " (2, 0.019120276543562995),\n",
      " (3, 0.023312189215803015),\n",
      " (4, 0.07177692380940279),\n",
      " (5, 0.012376338229759715),\n",
      " (6, 0.012652765926049652),\n",
      " (7, 0.015681833239163178),\n",
      " (8, 0.022752553413297974),\n",
      " (9, 0.027595984325063022),\n",
      " (10, 0.05605640784415791),\n",
      " (11, 0.055234263426146644),\n",
      " (12, 0.029834588795734765),\n",
      " (13, 0.03757301028755038),\n",
      " (14, 0.04175927604924723),\n",
      " (15, 0.046873736806329634),\n",
      " (16, 0.028261189971447376),\n",
      " (17, 0.032253677145107414),\n",
      " (18, 0.015158469095917521),\n",
      " (19, 0.009910816499937069),\n",
      " (20, 0.06389342674765507),\n",
      " (21, 0.04649554154969994),\n",
      " (22, 0.044285416005865226),\n",
      " (23, 0.027833764343638497),\n",
      " (24, 0.023250503546463713),\n",
      " (25, 0.03779800986995305),\n",
      " (26, 0.04946093135473547),\n",
      " (27, 0.0786047603506468),\n",
      " (28, 0.02922961188090144),\n",
      " (29, 0.02138290697692375),\n",
      " (30, 0.03995684959520006),\n",
      " (31, 0.023573953955485928),\n",
      " (32, 0.0321408155994278),\n",
      " (33, 0.013108675965096848),\n",
      " (34, 0.04804888404574625),\n",
      " (35, 0.026643756295786036),\n",
      " (36, 0.03774496889516698),\n",
      " (37, 0.02820288714869866),\n",
      " (38, 0.05359100101195448),\n",
      " (39, 0.012004470079129598),\n",
      " (40, 0.03785137133595704),\n",
      " (41, 0.047394414625869595),\n",
      " (42, 0.015323931467701325),\n",
      " (43, 0.04988528831633517),\n",
      " (44, 0.05780004895742308),\n",
      " (45, 0.02530607957682975),\n",
      " (46, 0.013362475769097248),\n",
      " (47, 0.052754038067894325),\n",
      " (48, 0.015530940941866894),\n",
      " (49, 0.019912069160089144),\n",
      " (50, 0.04088731946985065),\n",
      " (51, 0.013128192648741084),\n",
      " (52, 0.03900119794986026),\n",
      " (53, 0.013921517811172122),\n",
      " (54, 0.028835292107991496),\n",
      " (55, 0.02323003703497501),\n",
      " (56, 0.02150817568371135),\n",
      " (57, 0.10617446484421697),\n",
      " (58, 0.04529849922165387),\n",
      " (59, 0.014900013097375618),\n",
      " (60, 0.03679337150863408),\n",
      " (61, 0.033567826337736716),\n",
      " (62, 0.03610843180290698),\n",
      " (63, 0.031096111722954015),\n",
      " (64, 0.02718667586506468),\n",
      " (65, 0.02521602916763655),\n",
      " (66, 0.021575719383931883),\n",
      " (67, 0.029856207645587877),\n",
      " (68, 0.03655910589570113),\n",
      " (69, 0.015919816686982332),\n",
      " (70, 0.022958283416423714),\n",
      " (71, 0.05451664151643441),\n",
      " (72, 0.029109456343660196),\n",
      " (73, 0.008695441409733096),\n",
      " (74, 0.04932572363796693),\n",
      " (75, 0.04723810442577022),\n",
      " (76, 0.024766883443427493),\n",
      " (77, 0.029013854678344393),\n",
      " (78, 0.0155652040139616),\n",
      " (79, 0.025126887718245665),\n",
      " (80, 0.024155965230042462),\n",
      " (81, 0.024043175756275282),\n",
      " (82, 0.015857498817223165),\n",
      " (83, 0.01643485336109034),\n",
      " (84, 0.0235845876662888),\n",
      " (85, 0.030554074183528162),\n",
      " (86, 0.026949897720960334),\n",
      " (87, 0.03046086703068617),\n",
      " (88, 0.047394414625869595),\n",
      " (89, 0.032690238631608005),\n",
      " (90, 0.016782549056383145),\n",
      " (91, 0.023898670524856266),\n",
      " (92, 0.020118035533647896),\n",
      " (93, 0.01963563670924044),\n",
      " (94, 0.04119452594916113),\n",
      " (95, 0.043807602665850605),\n",
      " (96, 0.048573669846684646),\n",
      " (97, 0.037181622853869924),\n",
      " (98, 0.03906833064759097),\n",
      " (99, 0.04360582755603524),\n",
      " (100, 0.0382902838078568),\n",
      " (101, 0.1306418138299583),\n",
      " (102, 0.0489416937534973),\n",
      " (103, 0.05417027179886431),\n",
      " (104, 0.07966183720331212),\n",
      " (105, 0.020484836417520958),\n",
      " (106, 0.02450324309669606),\n",
      " (107, 0.08013840591073573),\n",
      " (108, 0.02168659176707966),\n",
      " (109, 0.04839525376331634),\n",
      " (110, 0.01896126440195416),\n",
      " (111, 0.018560753339895854),\n",
      " (112, 0.018780327252124314),\n",
      " (113, 0.015207783113798891),\n",
      " (114, 0.026856935854635264),\n",
      " (115, 0.04708451280219062),\n",
      " (116, 0.03043771814258307),\n",
      " (117, 0.024742588254089058),\n",
      " (118, 0.03915115011669634),\n",
      " (119, 0.017052609720696582),\n",
      " (120, 0.020789656866217858),\n",
      " (121, 0.02696548694964169),\n",
      " (122, 0.016525285096755275),\n",
      " (123, 0.015879704752369696),\n",
      " (124, 0.038693083722250485),\n",
      " (125, 0.015928755246201673),\n",
      " (126, 0.012880945170284807),\n",
      " (127, 0.05928309034357077),\n",
      " (128, 0.042600741803649596),\n",
      " (129, 0.028311247965315534),\n",
      " (130, 0.01948052023398504),\n",
      " (131, 0.02655394573443737),\n",
      " (132, 0.02937189129958177),\n",
      " (133, 0.01993315463549717),\n",
      " (134, 0.009953901607692944),\n",
      " (135, 0.021609685947937697),\n",
      " (136, 0.011639372477309024),\n",
      " (137, 0.04037415379658188),\n",
      " (138, 0.04201282348544609),\n",
      " (139, 0.028022062003101696),\n",
      " (140, 0.027170691614991048),\n",
      " (141, 0.029965102931037466),\n",
      " (142, 0.029129369082096217),\n",
      " (143, 0.13547797540969225),\n",
      " (144, 0.029468842901083942),\n",
      " (145, 0.021500534122414608),\n",
      " (146, 0.012325643500707261),\n",
      " (147, 0.008217750288243904),\n",
      " (148, 0.05902789367903279),\n",
      " (149, 0.020969813714450152),\n",
      " (150, 0.012727135171893635),\n",
      " (151, 0.032690238631608005),\n",
      " (152, 0.017645959968960512),\n",
      " (153, 0.015813253755964648),\n",
      " (154, 0.034753558596896356),\n",
      " (155, 0.01447652697953196),\n",
      " (156, 0.012852492787774247),\n",
      " (157, 0.030277352983160526),\n",
      " (158, 0.030376592437764246),\n",
      " (159, 0.02444440679809556),\n",
      " (160, 0.0497272153091533),\n",
      " (161, 0.22512225351497783),\n",
      " (162, 0.2053033654920315),\n",
      " (163, 0.027075388937372765),\n",
      " (164, 0.041273026872751535),\n",
      " (165, 0.05949727753379023),\n",
      " (166, 0.04622888118592409),\n",
      " (167, 0.017206290439040272),\n",
      " (168, 0.020851904142462296),\n",
      " (169, 0.034905132404735736),\n",
      " (170, 0.0662545616184923),\n",
      " (171, 0.020903735126912303),\n",
      " (172, 0.04693354680543424),\n",
      " (173, 0.017134239840924725),\n",
      " (174, 0.05417027179886431),\n",
      " (175, 0.035317279592637287),\n",
      " (176, 0.008806528355636866),\n",
      " (177, 0.01096684233870625),\n",
      " (178, 0.03110998901125032),\n",
      " (179, 0.02300788096155906),\n",
      " (180, 0.04094378755637981),\n",
      " (181, 0.07591814279891779),\n",
      " (182, 0.0394294513713008),\n",
      " (183, 0.009569146270743768),\n",
      " (184, 0.08251326036916103),\n",
      " (185, 0.013282584120555864),\n",
      " (186, 0.055530342820843105),\n",
      " (187, 0.02530607957682975),\n",
      " (188, 0.02191260182767926),\n",
      " (189, 0.038576111207481896),\n",
      " (190, 0.038403507099153744),\n",
      " (191, 0.0340241517534227),\n",
      " (192, 0.00988145786279479),\n",
      " (193, 0.07377769181226108),\n",
      " (194, 0.03141592270434191),\n",
      " (195, 0.017915057540237058),\n",
      " (196, 0.12133698796480434),\n",
      " (197, 0.008460081860547596),\n",
      " (198, 0.026749702845408316),\n",
      " (199, 0.012986061924348643),\n",
      " (200, 0.052064793539621304),\n",
      " (201, 0.254199039171489),\n",
      " (202, 0.10254549467895628),\n",
      " (203, 0.029921383555835204),\n",
      " (204, 0.032750112052500724),\n",
      " (205, 0.02457414470185995),\n",
      " (206, 0.013335764835298473),\n",
      " (207, 0.018854748724180206),\n",
      " (208, 0.02838423429606825),\n",
      " (209, 0.047394414625869595),\n",
      " (210, 0.03790505718976452),\n",
      " (211, 0.026132289391989354),\n",
      " (212, 0.0497272153091533),\n",
      " (213, 0.04804888404574625),\n",
      " (214, 0.04493301413784412),\n",
      " (215, 0.033212589220686276),\n",
      " (216, 0.023876647332009615),\n",
      " (217, 0.0244326864304101),\n",
      " (218, 0.03394345445024107),\n",
      " (219, 0.012880945170284807),\n",
      " (220, 0.02505119043403536),\n",
      " (221, 0.03491819920870241),\n",
      " (222, 0.034041649318633974),\n",
      " (223, 0.035763752123760345),\n",
      " (224, 0.012989266894717619),\n",
      " (225, 0.04370911874234367),\n",
      " (226, 0.045810601843972835),\n",
      " (227, 0.03129423113972537),\n",
      " (228, 0.038967800684152856),\n",
      " (229, 0.047394414625869595),\n",
      " (230, 0.032197066081031664),\n",
      " (231, 0.027595984325063022),\n",
      " (232, 0.07543047845881641),\n",
      " (233, 0.01791058788021686),\n",
      " (234, 0.023343194381956082),\n",
      " (235, 0.028274979759291254),\n",
      " (236, 0.02017570950682058),\n",
      " (237, 0.016014125852156905),\n",
      " (238, 0.029474941427469942),\n",
      " (239, 0.04088731946985065),\n",
      " (240, 0.020312483258667455),\n",
      " (241, 0.030961560378612748),\n",
      " (242, 0.018518685162162297),\n",
      " (243, 0.24689500201959175),\n",
      " (244, 0.011152337283656931),\n",
      " (245, 0.015428942166978024),\n",
      " (246, 0.014130594985723688),\n",
      " (247, 0.021150768405666053),\n",
      " (248, 0.02579794173596304),\n",
      " (249, 0.024270213857752555),\n",
      " (250, 0.026810819322922813),\n",
      " (251, 0.021145646225576013),\n",
      " (252, 0.014591071980333967),\n",
      " (253, 0.03567964390940094),\n",
      " (254, 0.02218986021219747),\n",
      " (255, 0.007057423536264057),\n",
      " (256, 0.09773559811866078),\n",
      " (257, 0.05563313219829627),\n",
      " (258, 0.01907546378319336),\n",
      " (259, 0.014931909459831236),\n",
      " (260, 0.04320433588484887),\n",
      " (261, 0.051545155681559526),\n",
      " (262, 0.01831129867729204),\n",
      " (263, 0.02762956343682941),\n",
      " (264, 0.035776460315007884),\n",
      " (265, 0.03779800986995305),\n",
      " (266, 0.017134239840924725),\n",
      " (267, 0.22916358608012724),\n",
      " (268, 0.36644981964937545),\n",
      " (269, 0.029856207645587877),\n",
      " (270, 0.24231371608045513),\n",
      " (271, 0.1337461666264441),\n",
      " (272, 0.032775254136080074),\n",
      " (273, 0.02786826163654623),\n",
      " (274, 0.0497272153091533),\n",
      " (275, 0.05999014444599241),\n",
      " (276, 0.03192920045996437),\n",
      " (277, 0.022235405933338368),\n",
      " (278, 0.017893814283997153),\n",
      " (279, 0.03597759539870593),\n",
      " (280, 0.02925145441524561),\n",
      " (281, 0.027331851119233048),\n",
      " (282, 0.04788051715048771),\n",
      " (283, 0.016141458789832128),\n",
      " (284, 0.017777282771738206),\n",
      " (285, 0.041842985640493664),\n",
      " (286, 0.031060235100751867),\n",
      " (287, 0.01868206425500239),\n",
      " (288, 0.013463367526215466),\n",
      " (289, 0.029149326821109333),\n",
      " (290, 0.05792254094600792),\n",
      " (291, 0.02456247400789372),\n",
      " (292, 0.015560913853199656),\n",
      " (293, 0.04932572363796693),\n",
      " (294, 0.01874950245379727),\n",
      " (295, 0.0319193294890811),\n",
      " (296, 0.022485496043596925),\n",
      " (297, 0.025894159139522144),\n",
      " (298, 0.049389932511378284),\n",
      " (299, 0.022535287346768587),\n",
      " (300, 0.015552339778977834),\n",
      " (301, 0.04151280528722778),\n",
      " (302, 0.05453406795738814),\n",
      " (303, 0.026628724299382664),\n",
      " (304, 0.012991927272701946),\n",
      " (305, 0.041273026872751535),\n",
      " (306, 0.020820725660811355),\n",
      " (307, 0.028663434443020025),\n",
      " (308, 0.007608080347794689),\n",
      " (309, 0.022560977663399328),\n",
      " (310, 0.024201487473903514),\n",
      " (311, 0.043106751513443245),\n",
      " (312, 0.07873137445204814),\n",
      " (313, 0.023499874347615617),\n",
      " (314, 0.010941384564505252),\n",
      " (315, 0.01964925474702721),\n",
      " (316, 0.23755407388394062),\n",
      " (317, 0.01163112278427121),\n",
      " (318, 0.0220994674501101),\n",
      " (319, 0.019301836742227598),\n",
      " (320, 0.023489342091442073),\n",
      " (321, 0.042452282093566586),\n",
      " (322, 0.03343698993353568),\n",
      " (323, 0.013456605431126447),\n",
      " (324, 0.05411920933817456),\n",
      " (325, 0.02065882046201682),\n",
      " (326, 0.0237890996930625),\n",
      " (327, 0.03194671337382753),\n",
      " (328, 0.035763752123760345),\n",
      " (329, 0.01746852157392519),\n",
      " (330, 0.013974576423957575),\n",
      " (331, 0.03703402116029891),\n",
      " (332, 0.031363666478326356),\n",
      " (333, 0.04771558804744726),\n",
      " (334, 0.07104559397915955),\n",
      " (335, 0.013069771515595071),\n",
      " (336, 0.03717111012853028),\n",
      " (337, 0.033502165843783),\n",
      " (338, 0.010077514297358358)]\n",
      "abil 0.023067990364550667\n",
      "accept 0.02138290697692375\n",
      "account 0.019120276543562995\n",
      "act 0.023312189215803015\n",
      "action 0.07177692380940279\n",
      "actual 0.012376338229759715\n",
      "add 0.012652765926049652\n",
      "administr 0.015681833239163178\n",
      "advis 0.022752553413297974\n",
      "affair 0.027595984325063022\n",
      "afloat 0.05605640784415791\n",
      "agenc 0.055234263426146644\n",
      "aggress 0.029834588795734765\n",
      "agre 0.03757301028755038\n",
      "aleppo 0.04175927604924723\n",
      "alleg 0.046873736806329634\n",
      "allow 0.028261189971447376\n",
      "alongsid 0.032253677145107414\n",
      "america 0.015158469095917521\n",
      "american 0.009910816499937069\n",
      "amount 0.06389342674765507\n",
      "annex 0.04649554154969994\n",
      "appear 0.044285416005865226\n",
      "appropri 0.027833764343638497\n",
      "approv 0.023250503546463713\n",
      "aris 0.03779800986995305\n",
      "arm 0.04946093135473547\n",
      "assad 0.0786047603506468\n",
      "assess 0.02922961188090144\n",
      "avail 0.02138290697692375\n",
      "bashar 0.03995684959520006\n",
      "battl 0.023573953955485928\n",
      "behalf 0.0321408155994278\n",
      "believ 0.013108675965096848\n",
      "besieg 0.04804888404574625\n",
      "bigger 0.026643756295786036\n",
      "bipartisan 0.03774496889516698\n",
      "bomb 0.02820288714869866\n",
      "brief 0.05359100101195448\n",
      "bring 0.012004470079129598\n",
      "bulli 0.03785137133595704\n",
      "burr 0.047394414625869595\n",
      "busi 0.015323931467701325\n",
      "call 0.04988528831633517\n",
      "campaign 0.05780004895742308\n",
      "cast 0.02530607957682975\n",
      "center 0.013362475769097248\n",
      "chairman 0.052754038067894325\n",
      "children 0.015530940941866894\n",
      "choos 0.019912069160089144\n",
      "chorus 0.04088731946985065\n",
      "citi 0.013128192648741084\n",
      "clinton 0.03900119794986026\n",
      "close 0.013921517811172122\n",
      "closer 0.028835292107991496\n",
      "colleagu 0.02323003703497501\n",
      "commit 0.02150817568371135\n",
      "committe 0.10617446484421697\n",
      "commod 0.04529849922165387\n",
      "communiti 0.014900013097375618\n",
      "compound 0.03679337150863408\n",
      "compromis 0.033567826337736716\n",
      "comput 0.03610843180290698\n",
      "concern 0.031096111722954015\n",
      "conclud 0.02718667586506468\n",
      "confid 0.02521602916763655\n",
      "confirm 0.021575719383931883\n",
      "confront 0.029856207645587877\n",
      "consensus 0.03655910589570113\n",
      "control 0.015919816686982332\n",
      "controversi 0.022958283416423714\n",
      "conundrum 0.05451664151643441\n",
      "cooper 0.029109456343660196\n",
      "countri 0.008695441409733096\n",
      "covert 0.04932572363796693\n",
      "crimea 0.04723810442577022\n",
      "crisi 0.024766883443427493\n",
      "critic 0.029013854678344393\n",
      "current 0.0155652040139616\n",
      "daili 0.025126887718245665\n",
      "damag 0.024155965230042462\n",
      "date 0.024043175756275282\n",
      "deal 0.015857498817223165\n",
      "decid 0.01643485336109034\n",
      "declin 0.0235845876662888\n",
      "defeat 0.030554074183528162\n",
      "defin 0.026949897720960334\n",
      "delay 0.03046086703068617\n",
      "delv 0.047394414625869595\n",
      "democraci 0.032690238631608005\n",
      "democrat 0.016782549056383145\n",
      "deni 0.023898670524856266\n",
      "design 0.020118035533647896\n",
      "despit 0.01963563670924044\n",
      "deter 0.04119452594916113\n",
      "determin 0.043807602665850605\n",
      "devin 0.048573669846684646\n",
      "dictat 0.037181622853869924\n",
      "difficult 0.03906833064759097\n",
      "dilemma 0.04360582755603524\n",
      "diminish 0.0382902838078568\n",
      "diplomat 0.1306418138299583\n",
      "discredit 0.0489416937534973\n",
      "dispel 0.05417027179886431\n",
      "doubt 0.07966183720331212\n",
      "econom 0.020484836417520958\n",
      "economi 0.02450324309669606\n",
      "elect 0.08013840591073573\n",
      "email 0.02168659176707966\n",
      "embattl 0.04839525376331634\n",
      "end 0.01896126440195416\n",
      "entir 0.018560753339895854\n",
      "evid 0.018780327252124314\n",
      "exampl 0.015207783113798891\n",
      "exchang 0.026856935854635264\n",
      "expel 0.04708451280219062\n",
      "extens 0.03043771814258307\n",
      "eye 0.024742588254089058\n",
      "face 0.03915115011669634\n",
      "fall 0.017052609720696582\n",
      "fear 0.020789656866217858\n",
      "fell 0.02696548694964169\n",
      "fight 0.016525285096755275\n",
      "final 0.015879704752369696\n",
      "flynn 0.038693083722250485\n",
      "focus 0.015928755246201673\n",
      "follow 0.012880945170284807\n",
      "forc 0.05928309034357077\n",
      "foreign 0.042600741803649596\n",
      "frank 0.028311247965315534\n",
      "free 0.01948052023398504\n",
      "frequent 0.02655394573443737\n",
      "function 0.02937189129958177\n",
      "fund 0.01993315463549717\n",
      "give 0.009953901607692944\n",
      "goal 0.021609685947937697\n",
      "govern 0.011639372477309024\n",
      "graham 0.04037415379658188\n",
      "grasp 0.04201282348544609\n",
      "great 0.028022062003101696\n",
      "greater 0.027170691614991048\n",
      "greatest 0.029965102931037466\n",
      "guid 0.029129369082096217\n",
      "hack 0.13547797540969225\n",
      "hand 0.029468842901083942\n",
      "happen 0.021500534122414608\n",
      "hear 0.012325643500707261\n",
      "help 0.008217750288243904\n",
      "herring 0.05902789367903279\n",
      "hillari 0.020969813714450152\n",
      "hold 0.012727135171893635\n",
      "holiday 0.032690238631608005\n",
      "hope 0.017645959968960512\n",
      "hour 0.015813253755964648\n",
      "hous 0.034753558596896356\n",
      "idea 0.01447652697953196\n",
      "import 0.012852492787774247\n",
      "impos 0.030277352983160526\n",
      "includ 0.030376592437764246\n",
      "incom 0.02444440679809556\n",
      "ineffect 0.0497272153091533\n",
      "intellig 0.22512225351497783\n",
      "interfer 0.2053033654920315\n",
      "invit 0.027075388937372765\n",
      "ironi 0.041273026872751535\n",
      "isi 0.05949727753379023\n",
      "issu 0.04622888118592409\n",
      "john 0.017206290439040272\n",
      "join 0.020851904142462296\n",
      "keep 0.034905132404735736\n",
      "leader 0.0662545616184923\n",
      "leav 0.020903735126912303\n",
      "legitimaci 0.04693354680543424\n",
      "level 0.017134239840924725\n",
      "lindsay 0.05417027179886431\n",
      "littl 0.035317279592637287\n",
      "live 0.008806528355636866\n",
      "long 0.01096684233870625\n",
      "longtim 0.03110998901125032\n",
      "look 0.02300788096155906\n",
      "major 0.04094378755637981\n",
      "mccain 0.07591814279891779\n",
      "mcconnel 0.0394294513713008\n",
      "mean 0.009569146270743768\n",
      "measur 0.08251326036916103\n",
      "member 0.013282584120555864\n",
      "mike 0.055530342820843105\n",
      "minist 0.02530607957682975\n",
      "miss 0.02191260182767926\n",
      "mitch 0.038576111207481896\n",
      "mitt 0.038403507099153744\n",
      "moment 0.0340241517534227\n",
      "month 0.00988145786279479\n",
      "moscow 0.07377769181226108\n",
      "move 0.03141592270434191\n",
      "nation 0.017915057540237058\n",
      "nato 0.12133698796480434\n",
      "need 0.008460081860547596\n",
      "neighbor 0.026749702845408316\n",
      "news 0.012986061924348643\n",
      "nune 0.052064793539621304\n",
      "obama 0.254199039171489\n",
      "occas 0.10254549467895628\n",
      "occasion 0.029921383555835204\n",
      "odd 0.032750112052500724\n",
      "offic 0.02457414470185995\n",
      "offici 0.013335764835298473\n",
      "oper 0.018854748724180206\n",
      "oppon 0.02838423429606825\n",
      "outgo 0.047394414625869595\n",
      "oval 0.03790505718976452\n",
      "overal 0.026132289391989354\n",
      "overdu 0.0497272153091533\n",
      "overst 0.04804888404574625\n",
      "overt 0.04493301413784412\n",
      "packag 0.033212589220686276\n",
      "page 0.023876647332009615\n",
      "pain 0.0244326864304101\n",
      "parti 0.03394345445024107\n",
      "past 0.012880945170284807\n",
      "paul 0.02505119043403536\n",
      "penc 0.03491819920870241\n",
      "perceiv 0.034041649318633974\n",
      "philosophi 0.035763752123760345\n",
      "play 0.012989266894717619\n",
      "pois 0.04370911874234367\n",
      "poland 0.045810601843972835\n",
      "polici 0.03129423113972537\n",
      "polit 0.038967800684152856\n",
      "politic 0.047394414625869595\n",
      "portion 0.032197066081031664\n",
      "prefer 0.027595984325063022\n",
      "presid 0.07543047845881641\n",
      "presidenti 0.01791058788021686\n",
      "price 0.023343194381956082\n",
      "prime 0.028274979759291254\n",
      "privat 0.02017570950682058\n",
      "program 0.016014125852156905\n",
      "promin 0.029474941427469942\n",
      "provoc 0.04088731946985065\n",
      "public 0.020312483258667455\n",
      "punish 0.030961560378612748\n",
      "push 0.018518685162162297\n",
      "putin 0.24689500201959175\n",
      "question 0.011152337283656931\n",
      "real 0.015428942166978024\n",
      "reason 0.014130594985723688\n",
      "recent 0.021150768405666053\n",
      "recommend 0.02579794173596304\n",
      "refus 0.024270213857752555\n",
      "reject 0.026810819322922813\n",
      "relationship 0.021145646225576013\n",
      "releas 0.014591071980333967\n",
      "reluct 0.03567964390940094\n",
      "repeat 0.02218986021219747\n",
      "report 0.007057423536264057\n",
      "republican 0.09773559811866078\n",
      "resist 0.05563313219829627\n",
      "respond 0.01907546378319336\n",
      "respons 0.014931909459831236\n",
      "retali 0.04320433588484887\n",
      "retribut 0.051545155681559526\n",
      "return 0.01831129867729204\n",
      "richard 0.02762956343682941\n",
      "role 0.035776460315007884\n",
      "romney 0.03779800986995305\n",
      "rule 0.017134239840924725\n",
      "russia 0.22916358608012724\n",
      "russian 0.36644981964937545\n",
      "ryan 0.029856207645587877\n",
      "sanction 0.24231371608045513\n",
      "scarc 0.1337461666264441\n",
      "secur 0.032775254136080074\n",
      "select 0.02786826163654623\n",
      "sen 0.0497272153091533\n",
      "senat 0.05999014444599241\n",
      "send 0.03192920045996437\n",
      "senior 0.022235405933338368\n",
      "sens 0.017893814283997153\n",
      "server 0.03597759539870593\n",
      "servic 0.02925145441524561\n",
      "sever 0.027331851119233048\n",
      "show 0.04788051715048771\n",
      "sign 0.016141458789832128\n",
      "similar 0.017777282771738206\n",
      "slat 0.041842985640493664\n",
      "smart 0.031060235100751867\n",
      "sound 0.01868206425500239\n",
      "speak 0.013463367526215466\n",
      "speaker 0.029149326821109333\n",
      "spurn 0.05792254094600792\n",
      "stage 0.02456247400789372\n",
      "stand 0.015560913853199656\n",
      "steadfast 0.04932572363796693\n",
      "strong 0.01874950245379727\n",
      "stronger 0.0319193294890811\n",
      "subject 0.022485496043596925\n",
      "sudden 0.025894159139522144\n",
      "suggest 0.049389932511378284\n",
      "support 0.022535287346768587\n",
      "sure 0.015552339778977834\n",
      "surrog 0.04151280528722778\n",
      "syria 0.05453406795738814\n",
      "system 0.026628724299382664\n",
      "take 0.012991927272701946\n",
      "talli 0.041273026872751535\n",
      "target 0.020820725660811355\n",
      "theme 0.028663434443020025\n",
      "thing 0.007608080347794689\n",
      "threat 0.022560977663399328\n",
      "tie 0.024201487473903514\n",
      "tongu 0.043106751513443245\n",
      "tougher 0.07873137445204814\n",
      "trade 0.023499874347615617\n",
      "tri 0.010941384564505252\n",
      "true 0.01964925474702721\n",
      "trump 0.23755407388394062\n",
      "turn 0.01163112278427121\n",
      "tweet 0.0220994674501101\n",
      "twitter 0.019301836742227598\n",
      "typic 0.023489342091442073\n",
      "ukrain 0.042452282093566586\n",
      "undermin 0.03343698993353568\n",
      "unit 0.013456605431126447\n",
      "urg 0.05411920933817456\n",
      "usual 0.02065882046201682\n",
      "vice 0.0237890996930625\n",
      "visibl 0.03194671337382753\n",
      "vladimir 0.035763752123760345\n",
      "vote 0.01746852157392519\n",
      "want 0.013974576423957575\n",
      "warrant 0.03703402116029891\n",
      "washington 0.031363666478326356\n",
      "weaker 0.04771558804744726\n",
      "week 0.07104559397915955\n",
      "white 0.013069771515595071\n",
      "world 0.03717111012853028\n",
      "worldwid 0.033502165843783\n",
      "write 0.010077514297358358\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF weights words based on how often they appear in a document \n",
    "# versus how often they appear in the entire corpus\n",
    "# this helps LDA distinguish topics by weighting more important words higher\n",
    "\n",
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "\n",
    "# preview of how this works\n",
    "pprint(corpus_tfidf[0])\n",
    "for i in range(len(bow_corpus[0])):\n",
    "    print(dictionary[bow_corpus[0][i][0]], corpus_tfidf[0][i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: HDP goes here for num_topics\n",
    "# see https://medium.com/analytics-vidhya/text-classification-using-lda-35d5b98d4f05 for HDP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Words: 0.006*\"thing\" + 0.006*\"want\" + 0.004*\"feel\" + 0.004*\"live\" + 0.004*\"write\" + 0.004*\"book\" + 0.004*\"stori\" + 0.004*\"women\" + 0.004*\"life\" + 0.003*\"look\"\n",
      "Topic: 1 Words: 0.007*\"food\" + 0.004*\"famili\" + 0.004*\"want\" + 0.004*\"look\" + 0.003*\"live\" + 0.003*\"help\" + 0.003*\"need\" + 0.003*\"world\" + 0.003*\"start\" + 0.003*\"thing\"\n",
      "Topic: 2 Words: 0.020*\"trump\" + 0.008*\"clinton\" + 0.006*\"campaign\" + 0.005*\"presid\" + 0.005*\"polit\" + 0.005*\"state\" + 0.005*\"news\" + 0.004*\"candid\" + 0.004*\"report\" + 0.004*\"countri\"\n",
      "Topic: 3 Words: 0.012*\"health\" + 0.007*\"care\" + 0.006*\"patient\" + 0.005*\"state\" + 0.005*\"insur\" + 0.005*\"medic\" + 0.005*\"research\" + 0.004*\"studi\" + 0.004*\"hospit\" + 0.004*\"need\"\n",
      "Topic: 4 Words: 0.007*\"report\" + 0.006*\"attack\" + 0.004*\"kill\" + 0.003*\"state\" + 0.003*\"forc\" + 0.003*\"countri\" + 0.003*\"call\" + 0.003*\"world\" + 0.003*\"live\" + 0.003*\"group\"\n",
      "Topic: 5 Words: 0.012*\"school\" + 0.010*\"student\" + 0.007*\"state\" + 0.005*\"educ\" + 0.004*\"famili\" + 0.004*\"report\" + 0.003*\"help\" + 0.003*\"children\" + 0.003*\"presid\" + 0.003*\"program\"\n",
      "Topic: 6 Words: 0.009*\"polic\" + 0.008*\"report\" + 0.005*\"state\" + 0.005*\"offic\" + 0.004*\"citi\" + 0.004*\"trump\" + 0.003*\"shoot\" + 0.003*\"women\" + 0.003*\"presid\" + 0.003*\"take\"\n",
      "Topic: 7 Words: 0.011*\"trump\" + 0.007*\"percent\" + 0.006*\"state\" + 0.006*\"report\" + 0.006*\"countri\" + 0.005*\"presid\" + 0.004*\"studi\" + 0.004*\"american\" + 0.004*\"govern\" + 0.003*\"million\"\n",
      "Topic: 8 Words: 0.014*\"trump\" + 0.008*\"clinton\" + 0.007*\"state\" + 0.007*\"presid\" + 0.006*\"vote\" + 0.006*\"republican\" + 0.006*\"court\" + 0.005*\"democrat\" + 0.005*\"elect\" + 0.004*\"hous\"\n",
      "Topic: 9 Words: 0.008*\"music\" + 0.005*\"song\" + 0.005*\"trump\" + 0.004*\"album\" + 0.004*\"record\" + 0.004*\"play\" + 0.003*\"state\" + 0.003*\"band\" + 0.003*\"take\" + 0.003*\"perform\"\n"
     ]
    }
   ],
   "source": [
    "# training the model using the bow corpus\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} Words: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.004*\"trump\" + 0.002*\"clinton\" + 0.002*\"elect\" + 0.001*\"presid\" + 0.001*\"report\" + 0.001*\"campaign\" + 0.001*\"women\" + 0.001*\"polic\" + 0.001*\"state\" + 0.001*\"vote\"\n",
      "Topic: 1 Word: 0.004*\"trump\" + 0.003*\"zika\" + 0.002*\"climat\" + 0.001*\"virus\" + 0.001*\"hous\" + 0.001*\"women\" + 0.001*\"presid\" + 0.001*\"republican\" + 0.001*\"clinton\" + 0.001*\"mosquito\"\n",
      "Topic: 2 Word: 0.003*\"music\" + 0.002*\"song\" + 0.002*\"album\" + 0.002*\"school\" + 0.002*\"dylan\" + 0.002*\"parent\" + 0.001*\"health\" + 0.001*\"artist\" + 0.001*\"children\" + 0.001*\"care\"\n",
      "Topic: 3 Word: 0.003*\"trump\" + 0.002*\"polic\" + 0.001*\"report\" + 0.001*\"israel\" + 0.001*\"clinton\" + 0.001*\"state\" + 0.001*\"isra\" + 0.001*\"presid\" + 0.001*\"palestinian\" + 0.001*\"vote\"\n",
      "Topic: 4 Word: 0.003*\"refuge\" + 0.002*\"school\" + 0.002*\"food\" + 0.002*\"marijuana\" + 0.002*\"aleppo\" + 0.002*\"student\" + 0.001*\"song\" + 0.001*\"syrian\" + 0.001*\"trump\" + 0.001*\"music\"\n",
      "Topic: 5 Word: 0.003*\"trump\" + 0.002*\"dutert\" + 0.002*\"philippin\" + 0.002*\"pope\" + 0.001*\"clinton\" + 0.001*\"drug\" + 0.001*\"china\" + 0.001*\"presid\" + 0.001*\"vote\" + 0.001*\"women\"\n",
      "Topic: 6 Word: 0.006*\"health\" + 0.005*\"insur\" + 0.003*\"trump\" + 0.003*\"care\" + 0.003*\"abort\" + 0.003*\"patient\" + 0.002*\"coverag\" + 0.002*\"obamacar\" + 0.002*\"repeal\" + 0.002*\"plan\"\n",
      "Topic: 7 Word: 0.002*\"health\" + 0.002*\"food\" + 0.002*\"patient\" + 0.002*\"studi\" + 0.002*\"diseas\" + 0.001*\"music\" + 0.001*\"virus\" + 0.001*\"percent\" + 0.001*\"opioid\" + 0.001*\"research\"\n",
      "Topic: 8 Word: 0.006*\"trump\" + 0.004*\"clinton\" + 0.004*\"vote\" + 0.003*\"democrat\" + 0.003*\"elector\" + 0.003*\"republican\" + 0.002*\"percent\" + 0.002*\"state\" + 0.002*\"voter\" + 0.002*\"elect\"\n",
      "Topic: 9 Word: 0.002*\"trump\" + 0.002*\"drug\" + 0.002*\"vaccin\" + 0.001*\"food\" + 0.001*\"health\" + 0.001*\"women\" + 0.001*\"school\" + 0.001*\"farc\" + 0.001*\"book\" + 0.001*\"homeless\"\n"
     ]
    }
   ],
   "source": [
    "# training the model using the bow corpus\n",
    "\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oklahoma', 'citi', 'resid', 'wake', 'earli', 'year', 'magnitud', 'quak', 'earlier', 'week', 'magnitud', 'quak', 'strike', 'area', 'state', 'histor', 'know', 'earthquak', 'nell', 'greenfieldboyc', 'tell', 'newscast', 'unit', 'oklahoma', 'recent', 'see', 'dramat', 'rise', 'seismic', 'activ', 'think', 'state', 'associ', 'earthquak', 'probabl', 'california', 'think', 'oklahoma', 'oklahoma', 'high', 'quak', 'magnitud', 'greater', 'bust', 'record', 'top', 'previous', 'record', 'year', 'state', 'offici', 'say', 'rise', 'unlik', 'repres', 'natur', 'occur', 'process', 'concern', 'quak', 'link', 'drill', 'specif', 'wastewat', 'produc', 'drill', 'pump', 'deep', 'underground', 'dispos', 'well', 'oklahoma', 'tri', 'address', 'issu', 'coordin', 'council', 'seismic', 'activ', 'includ', 'regul', 'scientist', 'industri', 'repres', 'wertz', 'stateimpact', 'oklahoma', 'explain', 'connect', 'industri', 'increas', 'number', 'quak', 'weekend', 'edit', 'saturday', 'novemb', 'product', 'creat', 'toxic', 'wastewat', 'contamin', 'drink', 'water', 'compani', 'inject', 'fluid', 'underground', 'dispos', 'well', 'pressur', 'fault', 'caus', 'slip', 'scientist', 'respons', 'oklahoma', 'massiv', 'earthquak', 'spike', 'scientist', 'blame', 'industri', 'seismic', 'activ', 'research', 'earthquak', 'compromis', 'econom', 'vital', 'energi', 'stateimpact', 'say', 'nation', 'research', 'council', 'advis', 'investig', 'potenti', 'site', 'histori', 'earthquak', 'proxim', 'fault', 'line', 'scientist', 'suggest', 'compani', 'look', 'way', 'dispos', 'wastewat', 'altogeth']\n",
      "\n",
      "Score: 0.6416405439376831\t \n",
      "Topic: 0.009*\"polic\" + 0.008*\"report\" + 0.005*\"state\" + 0.005*\"offic\" + 0.004*\"citi\"\n",
      "\n",
      "Score: 0.24426712095737457\t \n",
      "Topic: 0.011*\"trump\" + 0.007*\"percent\" + 0.006*\"state\" + 0.006*\"report\" + 0.006*\"countri\"\n",
      "\n",
      "Score: 0.10904992371797562\t \n",
      "Topic: 0.012*\"health\" + 0.007*\"care\" + 0.006*\"patient\" + 0.005*\"state\" + 0.005*\"insur\"\n"
     ]
    }
   ],
   "source": [
    "# check how part of the training set is classified\n",
    "# first appearing topic is the one assigned to it\n",
    "print(processed_docs[4310])\n",
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.7830478549003601\t \n",
      "Topic: 0.004*\"trump\" + 0.002*\"clinton\" + 0.002*\"elect\" + 0.001*\"presid\" + 0.001*\"report\"\n",
      "\n",
      "Score: 0.15203642845153809\t \n",
      "Topic: 0.006*\"health\" + 0.005*\"insur\" + 0.003*\"trump\" + 0.003*\"care\" + 0.003*\"abort\"\n",
      "\n",
      "Score: 0.0598716102540493\t \n",
      "Topic: 0.002*\"health\" + 0.002*\"food\" + 0.002*\"patient\" + 0.002*\"studi\" + 0.002*\"diseas\"\n"
     ]
    }
   ],
   "source": [
    "# evaluate the tfidf version\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6995978355407715\t Topic: 0.006*\"thing\" + 0.006*\"want\" + 0.004*\"feel\" + 0.004*\"live\" + 0.004*\"write\"\n",
      "Score: 0.03338087350130081\t Topic: 0.008*\"music\" + 0.005*\"song\" + 0.005*\"trump\" + 0.004*\"album\" + 0.004*\"record\"\n",
      "Score: 0.03338008001446724\t Topic: 0.011*\"trump\" + 0.007*\"percent\" + 0.006*\"state\" + 0.006*\"report\" + 0.006*\"countri\"\n",
      "Score: 0.03337838500738144\t Topic: 0.007*\"report\" + 0.006*\"attack\" + 0.004*\"kill\" + 0.003*\"state\" + 0.003*\"forc\"\n",
      "Score: 0.03337789326906204\t Topic: 0.012*\"health\" + 0.007*\"care\" + 0.006*\"patient\" + 0.005*\"state\" + 0.005*\"insur\"\n",
      "Score: 0.03337752819061279\t Topic: 0.020*\"trump\" + 0.008*\"clinton\" + 0.006*\"campaign\" + 0.005*\"presid\" + 0.005*\"polit\"\n",
      "Score: 0.03337736800312996\t Topic: 0.014*\"trump\" + 0.008*\"clinton\" + 0.007*\"state\" + 0.007*\"presid\" + 0.006*\"vote\"\n",
      "Score: 0.033377304673194885\t Topic: 0.009*\"polic\" + 0.008*\"report\" + 0.005*\"state\" + 0.005*\"offic\" + 0.004*\"citi\"\n",
      "Score: 0.03337668627500534\t Topic: 0.012*\"school\" + 0.010*\"student\" + 0.007*\"state\" + 0.005*\"educ\" + 0.004*\"famili\"\n",
      "Score: 0.03337604179978371\t Topic: 0.007*\"food\" + 0.004*\"famili\" + 0.004*\"want\" + 0.004*\"look\" + 0.003*\"live\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'writing out the equation for Euler'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
