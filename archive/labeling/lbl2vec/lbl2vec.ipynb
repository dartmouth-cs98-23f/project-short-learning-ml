{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lbl2Vec Model\n",
    "\n",
    "## LDA Topic Inference\n",
    "\n",
    "Unsupervised text classification model. Requires sort of a dataset that can map terms to the correct clusters.\n",
    "Can be done manually during the training stage of the LDA. But implementing this in the case where it might out-perform human input (such as long topic lists).\n",
    "There has also been a paper showing that such models can perform better than LDAs; so if effective, the Lbl2vec model can be used for topic modeling directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups     # the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vars here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train = fetch_20newsgroups(subset='train', shuffle=False)\n",
    "test = fetch_20newsgroups(subset='test', shuffle=False)\n",
    "\n",
    "# parse data to pandas DataFrames\n",
    "newsgroup_train = pd.DataFrame({'article':train.data, 'class_index':train.target})\n",
    "newsgroup_test = pd.DataFrame({'article':test.data, 'class_index':test.target})\n",
    "\n",
    "# load labels with keywords\n",
    "labels = pd.read_csv('20newsgroups_keywords.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   class_index          class_name               keywords  number_of_keywords\n",
      "0            8     rec.motorcycles    [bikes, motorcycle]                   2\n",
      "1            9  rec.sport.baseball             [baseball]                   1\n",
      "2           10    rec.sport.hockey               [hockey]                   1\n",
      "3           11           sci.crypt  [encryption, privacy]                   2\n"
     ]
    }
   ],
   "source": [
    "# data pre-processing\n",
    "\n",
    "# split keywords by separator and save them as array\n",
    "labels['keywords'] = labels['keywords'].apply(lambda x: x.split(' '))\n",
    "\n",
    "# convert description keywords to lowercase\n",
    "labels['keywords'] = labels['keywords'].apply(lambda description_keywords: [keyword.lower() for keyword in description_keywords])\n",
    "\n",
    "# get number of keywords for each class\n",
    "labels['number_of_keywords'] = labels['keywords'].apply(lambda row: len(row))\n",
    "\n",
    "# lets check our keywords\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             article  class_index   \n",
      "0  From: cubbie@garnet.berkeley.edu (            ...            9  \\\n",
      "1  From: crypt-comments@math.ncsu.edu\\nSubject: C...           11   \n",
      "2  From: george@minster.york.ac.uk\\nSubject: Non-...           11   \n",
      "3  From: williac@govonca.gov.on.ca (Chris William...           10   \n",
      "4  From: ayari@judikael.loria.fr (Ayari Iskander)...           10   \n",
      "\n",
      "  data_set_type                                        tagged_docs doc_key   \n",
      "0         train  ([from, cubbie, garnet, berkeley, edu, subject...       0  \\\n",
      "1         train  ([from, crypt, comments, math, ncsu, edu, subj...       2   \n",
      "2         train  ([from, george, minster, york, ac, uk, subject...      11   \n",
      "3         train  ([from, williac, govonca, gov, on, ca, chris, ...      12   \n",
      "4         train  ([from, ayari, judikael, loria, fr, ayari, isk...      15   \n",
      "\n",
      "           class_name               keywords  number_of_keywords  \n",
      "0  rec.sport.baseball             [baseball]                   1  \n",
      "1           sci.crypt  [encryption, privacy]                   2  \n",
      "2           sci.crypt  [encryption, privacy]                   2  \n",
      "3    rec.sport.hockey               [hockey]                   1  \n",
      "4    rec.sport.hockey               [hockey]                   1  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import strip_tags\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "# doc: document text string\n",
    "# returns tokenized document\n",
    "# strip_tags removes meta tags from the text\n",
    "# simple preprocess converts a document into a list of lowercase tokens, ignoring tokens that are too short or too long \n",
    "# simple preprocess also removes numerical values as well as punktuation characters\n",
    "def tokenize(doc):\n",
    "    return simple_preprocess(strip_tags(doc), deacc=True, min_len=2, max_len=15)\n",
    "\n",
    "# add data set type column\n",
    "newsgroup_train['data_set_type'] = 'train'\n",
    "newsgroup_test['data_set_type'] = 'test'\n",
    "\n",
    "# concat train and test data\n",
    "newsgroup_full_corpus = pd.concat([newsgroup_train,newsgroup_test]).reset_index(drop=True)\n",
    "\n",
    "# reduce dataset to only articles that belong to classes where we defined our keywords\n",
    "newsgroup_full_corpus = newsgroup_full_corpus[newsgroup_full_corpus['class_index'].isin(list(labels['class_index']))]\n",
    "\n",
    "# tokenize and tag documents for Lbl2Vec training\n",
    "newsgroup_full_corpus['tagged_docs'] = newsgroup_full_corpus.apply(lambda row: TaggedDocument(tokenize(row['article']), \n",
    "    [str(row.name)]), axis=1)\n",
    "\n",
    "# add doc_key column\n",
    "newsgroup_full_corpus['doc_key'] = newsgroup_full_corpus.index.astype(str)\n",
    "\n",
    "# add class_name column\n",
    "newsgroup_full_corpus = newsgroup_full_corpus.merge(labels, left_on='class_index', right_on='class_index', how='left')\n",
    "print(newsgroup_full_corpus.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 15:23:06,383 - Lbl2Vec - INFO - Train document and word embeddings\n",
      "2023-11-06 15:23:11,240 - Lbl2Vec - INFO - Train label embeddings\n"
     ]
    }
   ],
   "source": [
    "from lbl2vec import Lbl2Vec\n",
    "\n",
    "# init model with parameters\n",
    "Lbl2Vec_model = Lbl2Vec(keywords_list=list(labels.keywords), tagged_documents=newsgroup_full_corpus['tagged_docs'][newsgroup_full_corpus['data_set_type'] == 'train'], label_names=list(labels.class_name), similarity_threshold=0.43, min_num_docs=100, epochs=10)\n",
    "\n",
    "# train model\n",
    "Lbl2Vec_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 15:23:11,349 - Lbl2Vec - INFO - Get document embeddings from model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 15:23:11,367 - Lbl2Vec - INFO - Calculate document<->label similarities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       rec.sport.baseball\n",
      "1                sci.crypt\n",
      "2                sci.crypt\n",
      "3       rec.sport.baseball\n",
      "4         rec.sport.hockey\n",
      "               ...        \n",
      "2385             sci.crypt\n",
      "2386      rec.sport.hockey\n",
      "2387             sci.crypt\n",
      "2388      rec.sport.hockey\n",
      "2389       rec.motorcycles\n",
      "Name: most_similar_label, Length: 2390, dtype: object\n",
      "F1 score: 0.8966527196652719\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# predict similarity scores\n",
    "model_docs_lbl_similarities = Lbl2Vec_model.predict_model_docs()\n",
    "\n",
    "# merge DataFrames to compare the predicted and true category labels\n",
    "evaluation_train = model_docs_lbl_similarities.merge(newsgroup_full_corpus[newsgroup_full_corpus['data_set_type'] == 'train'], left_on='doc_key', right_on='doc_key')\n",
    "y_true_train = evaluation_train['class_name']\n",
    "y_pred_train = evaluation_train['most_similar_label']\n",
    "print(y_pred_train)\n",
    "\n",
    "print('F1 score:',f1_score(y_true_train, y_pred_train, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 15:23:12,860 - Lbl2Vec - INFO - Calculate document embeddings\n",
      "2023-11-06 15:23:13,792 - Lbl2Vec - INFO - Calculate document<->label similarities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8849056603773585\n"
     ]
    }
   ],
   "source": [
    "# predict similarity scores of new test documents (they were not used during Lbl2Vec training)\n",
    "new_docs_lbl_similarities = Lbl2Vec_model.predict_new_docs(tagged_docs=newsgroup_full_corpus['tagged_docs'][newsgroup_full_corpus['data_set_type']=='test'])\n",
    "\n",
    "# merge DataFrames to compare the predicted and true topic labels\n",
    "evaluation_test = new_docs_lbl_similarities.merge(newsgroup_full_corpus[newsgroup_full_corpus['data_set_type']=='test'], left_on='doc_key', right_on='doc_key')\n",
    "y_true_test = evaluation_test['class_name']\n",
    "y_pred_test = evaluation_test['most_similar_label']\n",
    "\n",
    "print('F1 score:',f1_score(y_true_test, y_pred_test, average='micro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
